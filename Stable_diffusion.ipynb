{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stable_diffusion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmfJo0Quf+ymybCPsqjMvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Stable diffusion implementation on Google Colab**\n",
        "\n",
        "Official GitHub --> https://github.com/CompVis/stable-diffusion"
      ],
      "metadata": {
        "id": "0DFT7-at6vtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CompVis/stable-diffusion.git\n",
        "# !apt-get install python3-venv\n",
        "# !cd stable-diffusion && python3 -m venv ./testvenv && source testvenv/bin/activate && pip install -r requirements.txt\n",
        "# !cd stable-diffusion && source testvenv/bin/activate　python3 main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N-G0VbyqqA1",
        "outputId": "c049b971-1dc6-4d64-8989-713af52b63d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stable-diffusion'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Total 313 (delta 0), reused 0 (delta 0), pack-reused 313\u001b[K\n",
            "Receiving objects: 100% (313/313), 42.62 MiB | 21.10 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0kVpgT_6rC0",
        "outputId": "44be7851-8e73-404a-f986-9520617a9150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'stable-diffusion' already exists and is not an empty directory.\n",
            "/content/stable-diffusion\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CompVis/stable-diffusion.git\n",
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####Python3.7で作動\n",
        "# !sudo add-apt-repository -y ppa:deadsnakes/ppa\n",
        "# !sudo apt-get -y update\n",
        "# !sudo apt-get -y install python3.7\n",
        "# !sudo apt-get -y install python3.7-dev\n",
        "# !sudo apt-get -y install python3-pip\n",
        "# !sudo apt-get -y install python3.7-distutils\n",
        "# !python3.7 -m pip install --upgrade setuptools\n",
        "# !python3.7 -m pip install --upgrade pip\n",
        "# !python3.7 -m pip install --upgrade distlib\n",
        "# !sudo update-alternatives --set python /usr/bin/python3.7\n",
        "# !sudo ln -sf /usr/bin/python /usr/local/bin/python\n",
        "\n",
        "modules = \"\"\"\n",
        "name: ldm\n",
        "channels:\n",
        "  - pytorch\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - python=3.8.5\n",
        "  - pip=20.3\n",
        "  - cudatoolkit=11.3\n",
        "  - pytorch=1.11.0\n",
        "  - torchvision=0.12.0\n",
        "  - numpy=1.19.2\n",
        "  - pip:\n",
        "    - albumentations==0.4.3\n",
        "    - diffusers\n",
        "    - opencv-python==4.1.2.30\n",
        "    - pudb==2019.2\n",
        "    - invisible-watermark\n",
        "    - imageio==2.9.0\n",
        "    - imageio-ffmpeg==0.4.2\n",
        "    - pytorch-lightning==1.5.0\n",
        "    - omegaconf==2.1.1\n",
        "    - test-tube>=0.7.5\n",
        "    - streamlit>=0.73.1\n",
        "    - einops==0.3.0\n",
        "    - torch-fidelity==0.3.0\n",
        "    - transformers==4.19.2\n",
        "    - torchmetrics==0.6.0\n",
        "    - kornia==0.6\n",
        "    - -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "    - -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "    - -e .\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# modules = \"\"\"\n",
        "# pip==20.3\n",
        "# torchvision==0.12.0\n",
        "# numpy==1.19.2\n",
        "# albumentations==0.4.3\n",
        "# diffusers\n",
        "# opencv-python==4.1.2.30\n",
        "# pudb==2019.2\n",
        "# invisible-watermark\n",
        "# imageio==2.9.0\n",
        "# imageio-ffmpeg==0.4.2\n",
        "# omegaconf==2.1.1\n",
        "# test-tube>=0.7.5\n",
        "# streamlit>=0.73.1\n",
        "# einops==0.3.0\n",
        "# torch-fidelity==0.3.0\n",
        "# transformers==4.19.2\n",
        "# torchmetrics==0.6.0\n",
        "# pytorch-lightning ==1.5.0\n",
        "# kornia==0.6\n",
        "# -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "# -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "# -e .\n",
        "# \"\"\"\n",
        "\n",
        "# modules = \"\"\"\n",
        "# pip\n",
        "# torchvision\n",
        "# numpy\n",
        "# albumentations\n",
        "# diffusers\n",
        "# opencv-python\n",
        "# pudb\n",
        "# invisible-watermark\n",
        "# imageio\n",
        "# imageio-ffmpeg\n",
        "# omegaconf\n",
        "# test-tube\n",
        "# streamlit\n",
        "# einops\n",
        "# torch-fidelity\n",
        "# transformers\n",
        "# torchmetrics\n",
        "# kornia\n",
        "# -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "# -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "# -e .\n",
        "# \"\"\"\n",
        "\n",
        "# !pip install --upgrade tensorboard\n",
        "# !pip install --upgrade pytorch-lightning\n",
        "\n",
        "with open(\"requirements.txt\", mode=\"w\") as f:\n",
        "    f.write(modules)\n",
        "print(f)\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss2uREgQ8Ery",
        "outputId": "ed085487-6be7-48ca-fbed-4246f3223118"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_io.TextIOWrapper name='requirements.txt' mode='w' encoding='UTF-8'>\n",
            "\u001b[31mERROR: Invalid requirement: 'name: ldm' (from line 2 of requirements.txt)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os, sys, datetime, glob, importlib, csv\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from packaging import version\n",
        "from omegaconf import OmegaConf\n",
        "from torch.utils.data import random_split, DataLoader, Dataset, Subset\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "\n",
        "from pytorch_lightning import seed_everything\n",
        "from pytorch_lightning.trainer import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, LearningRateMonitor\n",
        "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
        "from pytorch_lightning.utilities import rank_zero_info\n",
        "\n",
        "from ldm.data.base import Txt2ImgIterableBaseDataset\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def makedirs(path):\n",
        "    if os.path.exists():\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path)\n"
      ],
      "metadata": {
        "id": "hwddUrQH-iCK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = \"./log\"\n",
        "resumedir = \"./resume\"\n",
        "seed = 23\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# parser = get_parser()\n",
        "# parser = Trainer.add_argparse_args(parser)\n",
        "#opt, unknown = parser.parse_known_args()\n",
        "\n",
        "# if opt.name and opt.resume:\n",
        "#     raise ValueError(\n",
        "#         \"-n/--name and -r/--resume cannot be specified both.\"\n",
        "#         \"If you want to resume training in a new log folder, \"\n",
        "#         \"use -n/--name in combination with --resume_from_checkpoint\"\n",
        "#     )\n",
        "# if opt.resume:\n",
        "#     if not os.path.exists(opt.resume):\n",
        "#         raise ValueError(\"Cannot find {}\".format(opt.resume))\n",
        "#     if os.path.isfile(opt.resume):\n",
        "#         paths = opt.resume.split(\"/\")\n",
        "#         # idx = len(paths)-paths[::-1].index(\"logs\")+1\n",
        "#         # logdir = \"/\".join(paths[:idx])\n",
        "#         logdir = \"/\".join(paths[:-2])\n",
        "#         ckpt = opt.resume\n",
        "#     else:\n",
        "#         assert os.path.isdir(opt.resume), opt.resume\n",
        "#         logdir = opt.resume.rstrip(\"/\")\n",
        "#         ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
        "\n",
        "#     opt.resume_from_checkpoint = ckpt\n",
        "#     base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*.yaml\")))\n",
        "#     opt.base = base_configs + opt.base\n",
        "#     _tmp = logdir.split(\"/\")\n",
        "#     nowname = _tmp[-1]\n",
        "# else:\n",
        "#     if opt.name:\n",
        "#         name = \"_\" + opt.name\n",
        "#     elif opt.base:\n",
        "#         cfg_fname = os.path.split(opt.base[0])[-1]\n",
        "#         cfg_name = os.path.splitext(cfg_fname)[0]\n",
        "#         name = \"_\" + cfg_name\n",
        "#     else:\n",
        "#         name = \"\"\n",
        "#     nowname = now + name + opt.postfix\n",
        "#     logdir = os.path.join(opt.logdir, nowname)\n",
        "\n",
        "ckptdir = os.path.join(logdir, \"checkpoints\")\n",
        "cfgdir = os.path.join(logdir, \"configs\")\n",
        "seed_everything(seed)\n",
        "\n",
        "try:\n",
        "    # init and save configs\n",
        "    configs = [OmegaConf.load(cfg) for cfg in opt.base]\n",
        "    cli = OmegaConf.from_dotlist(unknown)\n",
        "    config = OmegaConf.merge(*configs, cli)\n",
        "    lightning_config = config.pop(\"lightning\", OmegaConf.create())\n",
        "    # merge trainer cli with config\n",
        "    trainer_config = lightning_config.get(\"trainer\", OmegaConf.create())\n",
        "    # default to ddp\n",
        "    trainer_config[\"accelerator\"] = \"ddp\"\n",
        "    for k in nondefault_trainer_args(opt):\n",
        "        trainer_config[k] = getattr(opt, k)\n",
        "    if not \"gpus\" in trainer_config:\n",
        "        del trainer_config[\"accelerator\"]\n",
        "        cpu = True\n",
        "    else:\n",
        "        gpuinfo = trainer_config[\"gpus\"]\n",
        "        print(f\"Running on GPUs {gpuinfo}\")\n",
        "        cpu = False\n",
        "    trainer_opt = argparse.Namespace(**trainer_config)\n",
        "    lightning_config.trainer = trainer_config\n",
        "\n",
        "    # model\n",
        "    model = instantiate_from_config(config.model)\n",
        "\n",
        "    # trainer and callbacks\n",
        "    trainer_kwargs = dict()\n",
        "\n",
        "    # default logger configs\n",
        "    default_logger_cfgs = {\n",
        "        \"wandb\": {\n",
        "            \"target\": \"pytorch_lightning.loggers.WandbLogger\",\n",
        "            \"params\": {\n",
        "                \"name\": nowname,\n",
        "                \"save_dir\": logdir,\n",
        "                \"offline\": opt.debug,\n",
        "                \"id\": nowname,\n",
        "            }\n",
        "        },\n",
        "        \"testtube\": {\n",
        "            \"target\": \"pytorch_lightning.loggers.TestTubeLogger\",\n",
        "            \"params\": {\n",
        "                \"name\": \"testtube\",\n",
        "                \"save_dir\": logdir,\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "    default_logger_cfg = default_logger_cfgs[\"testtube\"]\n",
        "    if \"logger\" in lightning_config:\n",
        "        logger_cfg = lightning_config.logger\n",
        "    else:\n",
        "        logger_cfg = OmegaConf.create()\n",
        "    logger_cfg = OmegaConf.merge(default_logger_cfg, logger_cfg)\n",
        "    trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)\n",
        "\n",
        "    # modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to\n",
        "    # specify which metric is used to determine best models\n",
        "    default_modelckpt_cfg = {\n",
        "        \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
        "        \"params\": {\n",
        "            \"dirpath\": ckptdir,\n",
        "            \"filename\": \"{epoch:06}\",\n",
        "            \"verbose\": True,\n",
        "            \"save_last\": True,\n",
        "        }\n",
        "    }\n",
        "    if hasattr(model, \"monitor\"):\n",
        "        print(f\"Monitoring {model.monitor} as checkpoint metric.\")\n",
        "        default_modelckpt_cfg[\"params\"][\"monitor\"] = model.monitor\n",
        "        default_modelckpt_cfg[\"params\"][\"save_top_k\"] = 3\n",
        "\n",
        "    if \"modelcheckpoint\" in lightning_config:\n",
        "        modelckpt_cfg = lightning_config.modelcheckpoint\n",
        "    else:\n",
        "        modelckpt_cfg =  OmegaConf.create()\n",
        "    modelckpt_cfg = OmegaConf.merge(default_modelckpt_cfg, modelckpt_cfg)\n",
        "    print(f\"Merged modelckpt-cfg: \\n{modelckpt_cfg}\")\n",
        "    if version.parse(pl.__version__) < version.parse('1.4.0'):\n",
        "        trainer_kwargs[\"checkpoint_callback\"] = instantiate_from_config(modelckpt_cfg)\n",
        "\n",
        "    # add callback which sets up log directory\n",
        "    default_callbacks_cfg = {\n",
        "        \"setup_callback\": {\n",
        "            \"target\": \"main.SetupCallback\",\n",
        "            \"params\": {\n",
        "                \"resume\": opt.resume,\n",
        "                \"now\": now,\n",
        "                \"logdir\": logdir,\n",
        "                \"ckptdir\": ckptdir,\n",
        "                \"cfgdir\": cfgdir,\n",
        "                \"config\": config,\n",
        "                \"lightning_config\": lightning_config,\n",
        "            }\n",
        "        },\n",
        "        \"image_logger\": {\n",
        "            \"target\": \"main.ImageLogger\",\n",
        "            \"params\": {\n",
        "                \"batch_frequency\": 750,\n",
        "                \"max_images\": 4,\n",
        "                \"clamp\": True\n",
        "            }\n",
        "        },\n",
        "        \"learning_rate_logger\": {\n",
        "            \"target\": \"main.LearningRateMonitor\",\n",
        "            \"params\": {\n",
        "                \"logging_interval\": \"step\",\n",
        "                # \"log_momentum\": True\n",
        "            }\n",
        "        },\n",
        "        \"cuda_callback\": {\n",
        "            \"target\": \"main.CUDACallback\"\n",
        "        },\n",
        "    }\n",
        "    if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "        default_callbacks_cfg.update({'checkpoint_callback': modelckpt_cfg})\n",
        "\n",
        "    if \"callbacks\" in lightning_config:\n",
        "        callbacks_cfg = lightning_config.callbacks\n",
        "    else:\n",
        "        callbacks_cfg = OmegaConf.create()\n",
        "\n",
        "    if 'metrics_over_trainsteps_checkpoint' in callbacks_cfg:\n",
        "        print(\n",
        "            'Caution: Saving checkpoints every n train steps without deleting. This might require some free space.')\n",
        "        default_metrics_over_trainsteps_ckpt_dict = {\n",
        "            'metrics_over_trainsteps_checkpoint':\n",
        "                {\"target\": 'pytorch_lightning.callbacks.ModelCheckpoint',\n",
        "                  'params': {\n",
        "                      \"dirpath\": os.path.join(ckptdir, 'trainstep_checkpoints'),\n",
        "                      \"filename\": \"{epoch:06}-{step:09}\",\n",
        "                      \"verbose\": True,\n",
        "                      'save_top_k': -1,\n",
        "                      'every_n_train_steps': 10000,\n",
        "                      'save_weights_only': True\n",
        "                  }\n",
        "                  }\n",
        "        }\n",
        "        default_callbacks_cfg.update(default_metrics_over_trainsteps_ckpt_dict)\n",
        "\n",
        "    callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\n",
        "    if 'ignore_keys_callback' in callbacks_cfg and hasattr(trainer_opt, 'resume_from_checkpoint'):\n",
        "        callbacks_cfg.ignore_keys_callback.params['ckpt_path'] = trainer_opt.resume_from_checkpoint\n",
        "    elif 'ignore_keys_callback' in callbacks_cfg:\n",
        "        del callbacks_cfg['ignore_keys_callback']\n",
        "\n",
        "    trainer_kwargs[\"callbacks\"] = [instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\n",
        "\n",
        "    trainer = Trainer.from_argparse_args(trainer_opt, **trainer_kwargs)\n",
        "    trainer.logdir = logdir  ###\n",
        "\n",
        "    # data\n",
        "    data = instantiate_from_config(config.data)\n",
        "    # NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
        "    # calling these ourselves should not be necessary but it is.\n",
        "    # lightning still takes care of proper multiprocessing though\n",
        "    data.prepare_data()\n",
        "    data.setup()\n",
        "    print(\"#### Data #####\")\n",
        "    for k in data.datasets:\n",
        "        print(f\"{k}, {data.datasets[k].__class__.__name__}, {len(data.datasets[k])}\")\n",
        "\n",
        "    # configure learning rate\n",
        "    bs, base_lr = config.data.params.batch_size, config.model.base_learning_rate\n",
        "    if not cpu:\n",
        "        ngpu = len(lightning_config.trainer.gpus.strip(\",\").split(','))\n",
        "    else:\n",
        "        ngpu = 1\n",
        "    if 'accumulate_grad_batches' in lightning_config.trainer:\n",
        "        accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches\n",
        "    else:\n",
        "        accumulate_grad_batches = 1\n",
        "    print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
        "    lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\n",
        "    if opt.scale_lr:\n",
        "        model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
        "        print(\n",
        "            \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
        "                model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
        "    else:\n",
        "        model.learning_rate = base_lr\n",
        "        print(\"++++ NOT USING LR SCALING ++++\")\n",
        "        print(f\"Setting learning rate to {model.learning_rate:.2e}\")\n",
        "\n",
        "\n",
        "    # allow checkpointing via USR1\n",
        "    def melk(*args, **kwargs):\n",
        "        # run all checkpoint hooks\n",
        "        if trainer.global_rank == 0:\n",
        "            print(\"Summoning checkpoint.\")\n",
        "            ckpt_path = os.path.join(ckptdir, \"last.ckpt\")\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "\n",
        "\n",
        "    def divein(*args, **kwargs):\n",
        "        if trainer.global_rank == 0:\n",
        "            import pudb;\n",
        "            pudb.set_trace()\n",
        "\n",
        "\n",
        "    import signal\n",
        "\n",
        "    signal.signal(signal.SIGUSR1, melk)\n",
        "    signal.signal(signal.SIGUSR2, divein)\n",
        "\n",
        "    # run\n",
        "    if opt.train:\n",
        "        try:\n",
        "            trainer.fit(model, data)\n",
        "        except Exception:\n",
        "            melk()\n",
        "            raise\n",
        "    if not opt.no_test and not trainer.interrupted:\n",
        "        trainer.test(model, data)\n",
        "except Exception:\n",
        "    if opt.debug and trainer.global_rank == 0:\n",
        "        try:\n",
        "            import pudb as debugger\n",
        "        except ImportError:\n",
        "            import pdb as debugger\n",
        "        debugger.post_mortem()\n",
        "    raise\n",
        "finally:\n",
        "    # move newly created debug project to debug_runs\n",
        "    if opt.debug and not opt.resume and trainer.global_rank == 0:\n",
        "        dst, name = os.path.split(logdir)\n",
        "        dst = os.path.join(dst, \"debug_runs\", name)\n",
        "        os.makedirs(os.path.split(dst)[0], exist_ok=True)\n",
        "        os.rename(logdir, dst)\n",
        "    if trainer.global_rank == 0:\n",
        "        print(trainer.profiler.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "ouyhQpIhKCAY",
        "outputId": "299300df-4029-4028-aaa8-74bb5c490b80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 23\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b5cd21938b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# init and save configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mcli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dotlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b5cd21938b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b5cd21938b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# move newly created debug project to debug_runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"debug_runs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
          ]
        }
      ]
    }
  ]
}