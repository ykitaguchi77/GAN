{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Hyperstyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperstyle**\n",
        "\n",
        "StyleGANの潜在変数推定を高精度に行う\n",
        "\n",
        "Original: https://github.com/yuval-alaluf/hyperstyle\n",
        "\n",
        "Implementation: http://cedro3.com/ai/hyperstyle/, https://github.com/cedro3/hyperstyle"
      ],
      "metadata": {
        "id": "JW3eCnGp5k16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## セットアップ"
      ],
      "metadata": {
        "id": "uvSMDwdmGMyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRzve9Y5DucV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'hyperstyle'\n",
        "\n",
        "# clone repo\n",
        "!git clone https://github.com/cedro3/hyperstyle.git $CODE_DIR\n",
        "\n",
        "# install ninja\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "\n",
        "# Import Packages\n",
        "import time\n",
        "import sys\n",
        "import pprint\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "\n",
        "from notebooks.notebook_utils import Downloader, HYPERSTYLE_PATHS, W_ENCODERS_PATHS, run_alignment\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_inversion\n",
        "from utils.domain_adaptation_utils import run_domain_adaptation\n",
        "from utils.model_utils import load_model, load_generator\n",
        "from function import *\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#オリジナルNotebooksフォルダを参照\n",
        "\n",
        "# download pretrained_models\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1NxGZfkE3THgEfPHbUoLPjCKfpWTo08V1', 'pretrained_models.zip', quiet=False)\n",
        "! unzip pretrained_models.zip\n",
        "\n",
        "\n",
        "# set expeiment data\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"faces\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"cars\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_cars.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/cars_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/car_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"afhq_wild\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_afhq_wild.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/afhq_wild_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/afhq_wild_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "experiment_type = 'faces' #今回は顔をやりますという宣言\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "\n",
        "\n",
        "# Load HyperStyle Model\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
        "print('Model successfully loaded!')\n",
        "\n",
        "\n",
        "# difine function\n",
        "def generate_mp4(out_name, images, kwargs):\n",
        "    writer = imageio.get_writer(out_name + '.mp4', **kwargs)\n",
        "    for image in tqdm(images):\n",
        "        writer.append_data(image)\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def get_latent_and_weight_deltas(inputs, net, opts):\n",
        "    opts.resize_outputs = False\n",
        "    opts.n_iters_per_batch = 5\n",
        "    with torch.no_grad():\n",
        "        _, latent, weights_deltas, _ = run_inversion(inputs.to(\"cuda\").float(), net, opts)\n",
        "    weights_deltas = [w[0] if w is not None else None for w in weights_deltas]\n",
        "    return latent, weights_deltas\n",
        "    \n",
        "\n",
        "def get_result_from_vecs(vectors_a, vectors_b, weights_deltas_a, weights_deltas_b, alpha):\n",
        "    results = []\n",
        "    for i in range(len(vectors_a)):\n",
        "        with torch.no_grad():\n",
        "            cur_vec = vectors_b[i] * alpha + vectors_a[i] * (1 - alpha)\n",
        "            cur_weight_deltas = interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha)\n",
        "            res = net.decoder([cur_vec],\n",
        "                              weights_deltas=cur_weight_deltas,\n",
        "                              randomize_noise=False,\n",
        "                              input_is_latent=True)[0]\n",
        "            results.append(res[0])\n",
        "    return results\n",
        "\n",
        "def interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha):\n",
        "    cur_weight_deltas = []\n",
        "    for weight_idx, w in enumerate(weights_deltas_a):\n",
        "        if w is not None:\n",
        "            delta = weights_deltas_b[weight_idx] * alpha + weights_deltas_a[weight_idx] * (1 - alpha)\n",
        "        else:\n",
        "            delta = None\n",
        "        cur_weight_deltas.append(delta)\n",
        "    return cur_weight_deltas\n",
        "    \n",
        "def show_mp4(filename, width):\n",
        "    mp4 = open(filename + '.mp4', 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=\"%d\" controls autoplay loop>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % (width, data_url)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title サンプル画像表示\n",
        "display_pic('./images/pic')"
      ],
      "metadata": {
        "id": "g9sedWGRg_Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title align処理\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "reset_folder('./images/align')\n",
        "files = sorted(glob.glob('./images/pic/*.jpg'))\n",
        "for file in tqdm(files):\n",
        "    aligned_image = run_alignment(file)\n",
        "    name = os.path.basename(file)\n",
        "    aligned_image.save('./images/align/'+name)\n",
        "    \n",
        "display_pic('./images/align')"
      ],
      "metadata": {
        "id": "-2agAxv5ShDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 反転の実行"
      ],
      "metadata": {
        "id": "T65aP6qDHlT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CFQyaD1Qqe3d",
        "outputId": "c84fde93-f5c2-4233-f34c-c90bf72bbbb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:37<00:00,  3.70s/it]\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "image_paths = sorted(glob.glob('./images/align/*.jpg'))\n",
        "\n",
        "in_images = []\n",
        "all_vecs = []\n",
        "all_weights_deltas = []\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "\n",
        "if experiment_type == \"cars\":\n",
        "    resize_amount = (512, 384)\n",
        "else:\n",
        "    resize_amount = (opts.output_size, opts.output_size)\n",
        "\n",
        "for image_path in image_paths:\n",
        "    #print(f'Working on {os.path.basename(image_path)}...')\n",
        "    original_image = Image.open(image_path) #Alignされた画像を開いて入力用に整形する\n",
        "    original_image = original_image.convert(\"RGB\")\n",
        "    input_image = img_transforms(original_image)\n",
        "    # get the weight deltas for each image\n",
        "    result_vec, weights_deltas = get_latent_and_weight_deltas(input_image.unsqueeze(0), net, opts)\n",
        "    all_vecs.append([result_vec])\n",
        "    all_weights_deltas.append(weights_deltas)\n",
        "    in_images.append(original_image.resize(resize_amount))\n",
        "\n",
        "n_transition = 25\n",
        "if experiment_type == \"cars\":\n",
        "    SIZE = 384\n",
        "else:\n",
        "    SIZE = opts.output_size\n",
        "\n",
        "images = []\n",
        "image_paths.append(image_paths[0])\n",
        "all_vecs.append(all_vecs[0])\n",
        "all_weights_deltas.append(all_weights_deltas[0])\n",
        "in_images.append(in_images[0])\n",
        "\n",
        "for i in tqdm(range(1, len(image_paths))):\n",
        "    if i == 0:\n",
        "        alpha_vals = [0] * 10 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
        "    else:\n",
        "        alpha_vals = [0] * 5 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
        "\n",
        "    for alpha in alpha_vals:\n",
        "        image_a = np.array(in_images[i - 1])\n",
        "        image_b = np.array(in_images[i])\n",
        "        image_joint = np.zeros_like(image_a)\n",
        "        up_to_row = int((SIZE - 1) * alpha)\n",
        "        if up_to_row > 0:\n",
        "            image_joint[:(up_to_row + 1), :, :] = image_b[((SIZE - 1) - up_to_row):, :, :]\n",
        "        if up_to_row < (SIZE - 1):\n",
        "            image_joint[up_to_row:, :, :] = image_a[:(SIZE - up_to_row), :, :]\n",
        "\n",
        "        result_image = get_result_from_vecs(all_vecs[i - 1], all_vecs[i],\n",
        "                                            all_weights_deltas[i - 1], all_weights_deltas[i],\n",
        "                                            alpha)[0]\n",
        "        if experiment_type == \"cars\":\n",
        "            result_image = result_image[:, 64:448, :]\n",
        "\n",
        "        output_im = tensor2im(result_image)\n",
        "        res = np.concatenate([image_joint, np.array(output_im)], axis=1)\n",
        "        images.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqvG0oJtsUWt"
      },
      "outputs": [],
      "source": [
        "#@title 動画の作成\n",
        "kwargs = {'fps': 15}\n",
        "save_path = \"./notebooks/animations\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "gif_path = os.path.join(save_path, f\"{experiment_type}_gif\")\n",
        "generate_mp4(gif_path, images, kwargs)\n",
        "show_mp4(gif_path, width=opts.output_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hyperstyle",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}