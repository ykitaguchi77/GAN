{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Hyperstyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperstyle**\n",
        "\n",
        "StyleGANの潜在変数推定を高精度に行う\n",
        "\n",
        "Original: https://github.com/yuval-alaluf/hyperstyle\n",
        "\n",
        "Implementation: http://cedro3.com/ai/hyperstyle/, https://github.com/cedro3/hyperstyle"
      ],
      "metadata": {
        "id": "JW3eCnGp5k16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRzve9Y5DucV",
        "outputId": "70e6c6d3-0ea4-4958-f4ce-23db9a65fa51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d9589a76c5aa4620bf651f81513bb3e8",
            "b14cb30a6e144c0facedb746b133c4fa",
            "f5f4d4e37eb447cf87b7449257129148",
            "e13c034c81e14cccb9379982f8869ec5",
            "c2eb7eb6057545c9ab0637017c080be3",
            "b1d3368cc92d44589699ff9d98522bf8",
            "34ec704834ed40a1a952cac4292262fe",
            "2cf9cf48aa20465bbda8dc310926e246",
            "00b766fca159432a8bd5f873caa1486d",
            "526c752edead4ef785250b3e8656ad8b",
            "89f1db38d51049b08b3a91ad60068dd7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hyperstyle'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 311 (delta 43), reused 37 (delta 37), pack-reused 259\u001b[K\n",
            "Receiving objects: 100% (311/311), 54.96 MiB | 29.23 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "--2022-07-04 01:51:37--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220704%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220704T015138Z&X-Amz-Expires=300&X-Amz-Signature=5cec86c3a1b5b4229a108cdda312d4b2d64161df20675d18c6d4008d5fbd85f1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-04 01:51:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220704%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220704T015138Z&X-Amz-Expires=300&X-Amz-Signature=5cec86c3a1b5b4229a108cdda312d4b2d64161df20675d18c6d4008d5fbd85f1&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-07-04 01:51:38 (8.51 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=94733ab5eb779fcef02dc98843fd6a53f17db1c3ca3cebe17b833b2740242d22\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NxGZfkE3THgEfPHbUoLPjCKfpWTo08V1\n",
            "To: /content/hyperstyle/pretrained_models.zip\n",
            "100%|██████████| 1.58G/1.58G [00:15<00:00, 102MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  pretrained_models.zip\n",
            "  inflating: pretrained_models/faces_w_encoder.pt  \n",
            "  inflating: pretrained_models/hyperstyle_ffhq.pt  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9589a76c5aa4620bf651f81513bb3e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading HyperStyle from checkpoint: ./pretrained_models/hyperstyle_ffhq.pt\n",
            "Loading pretrained W encoder...\n",
            "Using WEncoder\n",
            "Loading WEncoder from checkpoint: ./pretrained_models/faces_w_encoder.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ],
      "source": [
        "#@title セットアップ\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'hyperstyle'\n",
        "\n",
        "# clone repo\n",
        "!git clone https://github.com/cedro3/hyperstyle.git $CODE_DIR\n",
        "\n",
        "# install ninja\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "\n",
        "# Import Packages\n",
        "import time\n",
        "import sys\n",
        "import pprint\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "\n",
        "from notebooks.notebook_utils import Downloader, HYPERSTYLE_PATHS, W_ENCODERS_PATHS, run_alignment\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_inversion\n",
        "from utils.domain_adaptation_utils import run_domain_adaptation\n",
        "from utils.model_utils import load_model, load_generator\n",
        "from function import *\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#オリジナルNotebooksフォルダを参照\n",
        "\n",
        "# download pretrained_models\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1NxGZfkE3THgEfPHbUoLPjCKfpWTo08V1', 'pretrained_models.zip', quiet=False)\n",
        "! unzip pretrained_models.zip\n",
        "\n",
        "\n",
        "# set expeiment data\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"faces\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"cars\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_cars.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/cars_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/car_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"afhq_wild\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_afhq_wild.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/afhq_wild_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/afhq_wild_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}\n",
        "\n",
        "experiment_type = 'faces'\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "\n",
        "\n",
        "# Load HyperStyle Model\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
        "print('Model successfully loaded!')\n",
        "\n",
        "\n",
        "# difine function\n",
        "def generate_mp4(out_name, images, kwargs):\n",
        "    writer = imageio.get_writer(out_name + '.mp4', **kwargs)\n",
        "    for image in tqdm(images):\n",
        "        writer.append_data(image)\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def get_latent_and_weight_deltas(inputs, net, opts):\n",
        "    opts.resize_outputs = False\n",
        "    opts.n_iters_per_batch = 5\n",
        "    with torch.no_grad():\n",
        "        _, latent, weights_deltas, _ = run_inversion(inputs.to(\"cuda\").float(), net, opts)\n",
        "    weights_deltas = [w[0] if w is not None else None for w in weights_deltas]\n",
        "    return latent, weights_deltas\n",
        "    \n",
        "\n",
        "def get_result_from_vecs(vectors_a, vectors_b, weights_deltas_a, weights_deltas_b, alpha):\n",
        "    results = []\n",
        "    for i in range(len(vectors_a)):\n",
        "        with torch.no_grad():\n",
        "            cur_vec = vectors_b[i] * alpha + vectors_a[i] * (1 - alpha)\n",
        "            cur_weight_deltas = interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha)\n",
        "            res = net.decoder([cur_vec],\n",
        "                              weights_deltas=cur_weight_deltas,\n",
        "                              randomize_noise=False,\n",
        "                              input_is_latent=True)[0]\n",
        "            results.append(res[0])\n",
        "    return results\n",
        "\n",
        "def interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha):\n",
        "    cur_weight_deltas = []\n",
        "    for weight_idx, w in enumerate(weights_deltas_a):\n",
        "        if w is not None:\n",
        "            delta = weights_deltas_b[weight_idx] * alpha + weights_deltas_a[weight_idx] * (1 - alpha)\n",
        "        else:\n",
        "            delta = None\n",
        "        cur_weight_deltas.append(delta)\n",
        "    return cur_weight_deltas\n",
        "    \n",
        "def show_mp4(filename, width):\n",
        "    mp4 = open(filename + '.mp4', 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=\"%d\" controls autoplay loop>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % (width, data_url)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title サンプル画像表示\n",
        "display_pic('./images/pic')"
      ],
      "metadata": {
        "id": "g9sedWGRg_Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title align処理\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "reset_folder('./images/align')\n",
        "files = sorted(glob.glob('./images/pic/*.jpg'))\n",
        "for file in tqdm(files):\n",
        "    aligned_image = run_alignment(file)\n",
        "    name = os.path.basename(file)\n",
        "    aligned_image.save('./images/align/'+name)\n",
        "    \n",
        "display_pic('./images/align')"
      ],
      "metadata": {
        "id": "-2agAxv5ShDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFQyaD1Qqe3d",
        "outputId": "bc0bdac6-77eb-42d2-ef6c-03f3ff1e8f02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:35<00:00,  3.52s/it]\n"
          ]
        }
      ],
      "source": [
        "#@title 反転の実行\n",
        "import glob\n",
        "image_paths = sorted(glob.glob('./images/align/*.jpg'))\n",
        "\n",
        "in_images = []\n",
        "all_vecs = []\n",
        "all_weights_deltas = []\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "\n",
        "if experiment_type == \"cars\":\n",
        "    resize_amount = (512, 384)\n",
        "else:\n",
        "    resize_amount = (opts.output_size, opts.output_size)\n",
        "\n",
        "for image_path in image_paths:\n",
        "    #print(f'Working on {os.path.basename(image_path)}...')\n",
        "    original_image = Image.open(image_path)\n",
        "    original_image = original_image.convert(\"RGB\")\n",
        "    input_image = img_transforms(original_image)\n",
        "    # get the weight deltas for each image\n",
        "    result_vec, weights_deltas = get_latent_and_weight_deltas(input_image.unsqueeze(0), net, opts)\n",
        "    all_vecs.append([result_vec])\n",
        "    all_weights_deltas.append(weights_deltas)\n",
        "    in_images.append(original_image.resize(resize_amount))\n",
        "\n",
        "n_transition = 25\n",
        "if experiment_type == \"cars\":\n",
        "    SIZE = 384\n",
        "else:\n",
        "    SIZE = opts.output_size\n",
        "\n",
        "images = []\n",
        "image_paths.append(image_paths[0])\n",
        "all_vecs.append(all_vecs[0])\n",
        "all_weights_deltas.append(all_weights_deltas[0])\n",
        "in_images.append(in_images[0])\n",
        "\n",
        "for i in tqdm(range(1, len(image_paths))):\n",
        "    if i == 0:\n",
        "        alpha_vals = [0] * 10 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
        "    else:\n",
        "        alpha_vals = [0] * 5 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
        "\n",
        "    for alpha in alpha_vals:\n",
        "        image_a = np.array(in_images[i - 1])\n",
        "        image_b = np.array(in_images[i])\n",
        "        image_joint = np.zeros_like(image_a)\n",
        "        up_to_row = int((SIZE - 1) * alpha)\n",
        "        if up_to_row > 0:\n",
        "            image_joint[:(up_to_row + 1), :, :] = image_b[((SIZE - 1) - up_to_row):, :, :]\n",
        "        if up_to_row < (SIZE - 1):\n",
        "            image_joint[up_to_row:, :, :] = image_a[:(SIZE - up_to_row), :, :]\n",
        "\n",
        "        result_image = get_result_from_vecs(all_vecs[i - 1], all_vecs[i],\n",
        "                                            all_weights_deltas[i - 1], all_weights_deltas[i],\n",
        "                                            alpha)[0]\n",
        "        if experiment_type == \"cars\":\n",
        "            result_image = result_image[:, 64:448, :]\n",
        "\n",
        "        output_im = tensor2im(result_image)\n",
        "        res = np.concatenate([image_joint, np.array(output_im)], axis=1)\n",
        "        images.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqvG0oJtsUWt"
      },
      "outputs": [],
      "source": [
        "#@title 動画の作成\n",
        "kwargs = {'fps': 15}\n",
        "save_path = \"./notebooks/animations\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "gif_path = os.path.join(save_path, f\"{experiment_type}_gif\")\n",
        "generate_mp4(gif_path, images, kwargs)\n",
        "show_mp4(gif_path, width=opts.output_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hyperstyle",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9589a76c5aa4620bf651f81513bb3e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b14cb30a6e144c0facedb746b133c4fa",
              "IPY_MODEL_f5f4d4e37eb447cf87b7449257129148",
              "IPY_MODEL_e13c034c81e14cccb9379982f8869ec5"
            ],
            "layout": "IPY_MODEL_c2eb7eb6057545c9ab0637017c080be3"
          }
        },
        "b14cb30a6e144c0facedb746b133c4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d3368cc92d44589699ff9d98522bf8",
            "placeholder": "​",
            "style": "IPY_MODEL_34ec704834ed40a1a952cac4292262fe",
            "value": "100%"
          }
        },
        "f5f4d4e37eb447cf87b7449257129148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf9cf48aa20465bbda8dc310926e246",
            "max": 87319819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00b766fca159432a8bd5f873caa1486d",
            "value": 87319819
          }
        },
        "e13c034c81e14cccb9379982f8869ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_526c752edead4ef785250b3e8656ad8b",
            "placeholder": "​",
            "style": "IPY_MODEL_89f1db38d51049b08b3a91ad60068dd7",
            "value": " 83.3M/83.3M [00:00&lt;00:00, 187MB/s]"
          }
        },
        "c2eb7eb6057545c9ab0637017c080be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d3368cc92d44589699ff9d98522bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34ec704834ed40a1a952cac4292262fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cf9cf48aa20465bbda8dc310926e246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00b766fca159432a8bd5f873caa1486d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "526c752edead4ef785250b3e8656ad8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f1db38d51049b08b3a91ad60068dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}