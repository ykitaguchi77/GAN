{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Hyperstyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperstyle**\n",
        "\n",
        "StyleGANの潜在変数推定を高精度に行う\n",
        "\n",
        "Original: https://github.com/yuval-alaluf/hyperstyle\n",
        "\n",
        "Implementation: http://cedro3.com/ai/hyperstyle/, https://github.com/cedro3/hyperstyle"
      ],
      "metadata": {
        "id": "JW3eCnGp5k16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## セットアップ"
      ],
      "metadata": {
        "id": "uvSMDwdmGMyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QRzve9Y5DucV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a89b3d-f8fd-466e-fd58-3b53b9deb147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'hyperstyle' already exists and is not an empty directory.\n",
            "--2022-08-01 17:11:34--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220801T171134Z&X-Amz-Expires=300&X-Amz-Signature=fb3bbc1647fabbdc9837a85c8054b20aab93a766e5bbe62082718e6561f2d394&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-08-01 17:11:35--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220801%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220801T171134Z&X-Amz-Expires=300&X-Amz-Signature=fb3bbc1647fabbdc9837a85c8054b20aab93a766e5bbe62082718e6561f2d394&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip.3’\n",
            "\n",
            "ninja-linux.zip.3   100%[===================>]  76.03K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-08-01 17:11:35 (7.88 MB/s) - ‘ninja-linux.zip.3’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "replace /usr/local/bin/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NxGZfkE3THgEfPHbUoLPjCKfpWTo08V1\n",
            "To: /content/hyperstyle/pretrained_models.zip\n",
            "100%|██████████| 1.58G/1.58G [00:32<00:00, 49.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  pretrained_models.zip\n",
            "replace pretrained_models/faces_w_encoder.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: pretrained_models/faces_w_encoder.pt  \n",
            "replace pretrained_models/hyperstyle_ffhq.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: pretrained_models/hyperstyle_ffhq.pt  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading HyperStyle from checkpoint: ./pretrained_models/hyperstyle_ffhq.pt\n",
            "Loading pretrained W encoder...\n",
            "Using WEncoder\n",
            "Loading WEncoder from checkpoint: ./pretrained_models/faces_w_encoder.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'hyperstyle'\n",
        "\n",
        "# clone repo\n",
        "!git clone https://github.com/cedro3/hyperstyle.git $CODE_DIR\n",
        "\n",
        "# install ninja\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "\n",
        "# Import Packages\n",
        "import time\n",
        "import sys\n",
        "import pprint\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import imageio\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "\n",
        "from notebooks.notebook_utils import Downloader, HYPERSTYLE_PATHS, W_ENCODERS_PATHS, run_alignment\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_inversion\n",
        "from utils.domain_adaptation_utils import run_domain_adaptation\n",
        "from utils.model_utils import load_model, load_generator\n",
        "from function import * #function内のmoduleをそのままimportできる\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#オリジナルNotebooksフォルダを参照\n",
        "\n",
        "# download pretrained_models\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1NxGZfkE3THgEfPHbUoLPjCKfpWTo08V1', 'pretrained_models.zip', quiet=False)\n",
        "! unzip pretrained_models.zip\n",
        "\n",
        "\n",
        "# set expeiment data\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"faces\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"cars\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_cars.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/cars_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/car_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"afhq_wild\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_afhq_wild.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/afhq_wild_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/afhq_wild_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "experiment_type = 'faces' #今回は顔をやりますという宣言\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "\n",
        "\n",
        "# Load HyperStyle Model（netはモデル、optsは係数の数字）\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
        "print('Model successfully loaded!')\n",
        "\n",
        "\n",
        "# difine function\n",
        "def generate_mp4(out_name, images, kwargs):\n",
        "    writer = imageio.get_writer(out_name + '.mp4', **kwargs)\n",
        "    for image in tqdm(images):\n",
        "        writer.append_data(image)\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def get_latent_and_weight_deltas(inputs, net, opts):\n",
        "    opts.resize_outputs = False\n",
        "    opts.n_iters_per_batch = 5\n",
        "    with torch.no_grad():\n",
        "        _, latent, weights_deltas, _ = run_inversion(inputs.to(\"cuda\").float(), net, opts)\n",
        "    weights_deltas = [w[0] if w is not None else None for w in weights_deltas]\n",
        "    return latent, weights_deltas\n",
        "    \n",
        "\n",
        "def get_result_from_vecs(vectors_a, vectors_b, weights_deltas_a, weights_deltas_b, alpha):\n",
        "    results = []\n",
        "    for i in range(len(vectors_a)):\n",
        "        with torch.no_grad():\n",
        "            cur_vec = vectors_b[i] * alpha + vectors_a[i] * (1 - alpha)\n",
        "            cur_weight_deltas = interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha)\n",
        "            res = net.decoder([cur_vec],\n",
        "                              weights_deltas=cur_weight_deltas,\n",
        "                              randomize_noise=False,\n",
        "                              input_is_latent=True)[0]\n",
        "            results.append(res[0])\n",
        "    return results\n",
        "\n",
        "def interpolate_weight_deltas(weights_deltas_a, weights_deltas_b, alpha):\n",
        "    cur_weight_deltas = []\n",
        "    for weight_idx, w in enumerate(weights_deltas_a):\n",
        "        if w is not None:\n",
        "            delta = weights_deltas_b[weight_idx] * alpha + weights_deltas_a[weight_idx] * (1 - alpha)\n",
        "        else:\n",
        "            delta = None\n",
        "        cur_weight_deltas.append(delta)\n",
        "    return cur_weight_deltas\n",
        "    \n",
        "def show_mp4(filename, width):\n",
        "    mp4 = open(filename + '.mp4', 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=\"%d\" controls autoplay loop>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % (width, data_url)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title サンプル画像表示 { form-width: \"20%\" }\n",
        "#function.pyから\n",
        "display_pic('./images/pic')"
      ],
      "metadata": {
        "id": "g9sedWGRg_Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title align処理 { form-width: \"20%\" }\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "reset_folder('./images/align') #function.pyから。既存フォルダを消して作成し直す\n",
        "files = sorted(glob.glob('./images/pic/*.jpg'))\n",
        "for file in tqdm(files): #tqdm: プログレスバー\n",
        "    aligned_image = run_alignment(file) #顔をalignする\n",
        "    name = os.path.basename(file)\n",
        "    aligned_image.save('./images/align/'+name) #alignした画像を別フォルダに保存\n",
        "    \n",
        "display_pic('./images/align')"
      ],
      "metadata": {
        "id": "-2agAxv5ShDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 反転の実行"
      ],
      "metadata": {
        "id": "T65aP6qDHlT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CFQyaD1Qqe3d",
        "outputId": "02e61b0a-46e2-4639-cd73-11a9f0ea9912",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:37<00:00,  3.71s/it]\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "image_paths = sorted(glob.glob('./images/align/*.jpg')) #反転させたい元画像のリスト\n",
        "\n",
        "in_images = []\n",
        "all_vecs = []\n",
        "all_weights_deltas = []\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "\n",
        "if experiment_type == \"cars\":\n",
        "    resize_amount = (512, 384)\n",
        "else:\n",
        "    resize_amount = (opts.output_size, opts.output_size)\n",
        "\n",
        "for image_path in image_paths:\n",
        "    #print(f'Working on {os.path.basename(image_path)}...')\n",
        "    original_image = Image.open(image_path) #Alignされた画像を開いて入力用に整形する\n",
        "    original_image = original_image.convert(\"RGB\") #RGBに変換\n",
        "    input_image = img_transforms(original_image) #モデルに合わせて入力サイズを整える\n",
        "    # get the weight deltas for each image\n",
        "    result_vec, weights_deltas = get_latent_and_weight_deltas(input_image.unsqueeze(0), net, opts)\n",
        "    all_vecs.append([result_vec])\n",
        "    all_weights_deltas.append(weights_deltas)\n",
        "    in_images.append(original_image.resize(resize_amount))\n",
        "\n",
        "n_transition = 25\n",
        "if experiment_type == \"cars\":\n",
        "    SIZE = 384\n",
        "else:\n",
        "    SIZE = opts.output_size\n",
        "\n",
        "images = []\n",
        "image_paths.append(image_paths[0])\n",
        "all_vecs.append(all_vecs[0])\n",
        "all_weights_deltas.append(all_weights_deltas[0])\n",
        "in_images.append(in_images[0])\n",
        "\n",
        "for i in tqdm(range(1, len(image_paths))):\n",
        "    if i == 0:\n",
        "        alpha_vals = [0] * 10 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
        "    else:\n",
        "        alpha_vals = [0] * 5 + np.linspace(0, 1, n_transition).tolist() + [1] * 5\n",
        "\n",
        "    for alpha in alpha_vals:\n",
        "        image_a = np.array(in_images[i - 1])\n",
        "        image_b = np.array(in_images[i])\n",
        "        image_joint = np.zeros_like(image_a)\n",
        "        up_to_row = int((SIZE - 1) * alpha)\n",
        "        if up_to_row > 0:\n",
        "            image_joint[:(up_to_row + 1), :, :] = image_b[((SIZE - 1) - up_to_row):, :, :]\n",
        "        if up_to_row < (SIZE - 1):\n",
        "            image_joint[up_to_row:, :, :] = image_a[:(SIZE - up_to_row), :, :]\n",
        "\n",
        "        result_image = get_result_from_vecs(all_vecs[i - 1], all_vecs[i],\n",
        "                                            all_weights_deltas[i - 1], all_weights_deltas[i],\n",
        "                                            alpha)[0]\n",
        "        if experiment_type == \"cars\":\n",
        "            result_image = result_image[:, 64:448, :]\n",
        "\n",
        "        output_im = tensor2im(result_image)\n",
        "        res = np.concatenate([image_joint, np.array(output_im)], axis=1)\n",
        "        images.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqvG0oJtsUWt"
      },
      "outputs": [],
      "source": [
        "#@title 動画の作成 { form-width: \"20%\" }\n",
        "kwargs = {'fps': 15}\n",
        "save_path = \"./notebooks/animations\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "gif_path = os.path.join(save_path, f\"{experiment_type}_gif\")\n",
        "generate_mp4(gif_path, images, kwargs)\n",
        "show_mp4(gif_path, width=opts.output_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hyperstyle",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}