{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPmR1yAr0eCqZ2CXSC1VQkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/FaceSwap_sber-swap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**FaceSwap using SberSwap**\n",
        "\n",
        "implementation: http://cedro3.com/ai/sberswap/\n",
        "\n"
      ],
      "metadata": {
        "id": "T8hpBdt2gLFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llUExd5Qfl9n"
      },
      "outputs": [],
      "source": [
        "#@markdown #**セットアップ**\n",
        "\n",
        "# Clone github \n",
        "!git clone https://github.com/cedro3/sber-swap.git\n",
        "%cd sber-swap\n",
        "\n",
        "# load arcface\n",
        "!wget -P ./arcface_model https://github.com/sberbank-ai/sber-swap/releases/download/arcface/backbone.pth\n",
        "!wget -P ./arcface_model https://github.com/sberbank-ai/sber-swap/releases/download/arcface/iresnet.py\n",
        "\n",
        "# load landmarks detector\n",
        "!wget -P ./insightface_func/models/antelope https://github.com/sberbank-ai/sber-swap/releases/download/antelope/glintr100.onnx\n",
        "!wget -P ./insightface_func/models/antelope https://github.com/sberbank-ai/sber-swap/releases/download/antelope/scrfd_10g_bnkps.onnx\n",
        "\n",
        "# load model itself\n",
        "!wget -P ./weights https://github.com/sberbank-ai/sber-swap/releases/download/sber-swap-v2.0/G_unet_2blocks.pth\n",
        "\n",
        "# load super res model\n",
        "!wget -P ./weights https://github.com/sberbank-ai/sber-swap/releases/download/super-res/10_net_G.pth\n",
        "\n",
        "# Install required libraries\n",
        "!pip install mxnet-cu101mkl\n",
        "!pip install onnxruntime-gpu==1.8\n",
        "!pip install insightface==0.2.1\n",
        "!pip install kornia==0.5.4\n",
        "\n",
        "# library import\n",
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "from utils.inference.image_processing import crop_face, get_final_image, show_images\n",
        "from utils.inference.video_processing import read_video, get_target, get_final_video, add_audio_from_another_video, face_enhancement\n",
        "from utils.inference.core import model_inference\n",
        "from network.AEI_Net import AEI_Net\n",
        "from coordinate_reg.image_infer import Handler\n",
        "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
        "from arcface_model.iresnet import iresnet100\n",
        "from models.pix2pix_model import Pix2PixModel\n",
        "from models.config_sr import TestOptions\n",
        "\n",
        "\n",
        "# --- Initialize models ---\n",
        "app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
        "app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
        "\n",
        "# main model for generation\n",
        "G = AEI_Net(backbone='unet', num_blocks=2, c_id=512)\n",
        "G.eval()\n",
        "G.load_state_dict(torch.load('weights/G_unet_2blocks.pth', map_location=torch.device('cpu')))\n",
        "G = G.cuda()\n",
        "G = G.half()\n",
        "\n",
        "# arcface model to get face embedding\n",
        "netArc = iresnet100(fp16=False)\n",
        "netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))\n",
        "netArc=netArc.cuda()\n",
        "netArc.eval()\n",
        "\n",
        "# model to get face landmarks\n",
        "handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)\n",
        "\n",
        "# model to make superres of face, set use_sr=True if you want to use super resolution or use_sr=False if you don't\n",
        "use_sr = True\n",
        "if use_sr:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    opt = TestOptions()\n",
        "    #opt.which_epoch ='10_7'\n",
        "    model = Pix2PixModel(opt)\n",
        "    model.netG.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**静止画をfaceSwap**"
      ],
      "metadata": {
        "id": "CPUwCaCdoH6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c7oadGDPoLU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #**画像をFaceswap**\n",
        "#@markdown ・自分の画像は examples/images にアップロードして下さい\n",
        "source = 'satomi.jpg' #@param {type:\"string\"}\n",
        "target = 'nanako.jpg' #@param {type:\"string\"}\n",
        "source_path = 'examples/images/'+source\n",
        "target_path = 'examples/images/' + target\n",
        "\n",
        "source_full = cv2.imread(source_path)\n",
        "crop_size = 224 # don't change this\n",
        "batch_size =  40\n",
        "\n",
        "source = crop_face(source_full, app, crop_size)[0]\n",
        "source = [source[:, :, ::-1]]\n",
        "\n",
        "target_full = cv2.imread(target_path)\n",
        "full_frames = [target_full]\n",
        "target = get_target(full_frames, app, crop_size)\n",
        "\n",
        "final_frames_list, crop_frames_list, full_frames, tfm_array_list = model_inference(full_frames,\n",
        "                                                                                   source,\n",
        "                                                                                   target,\n",
        "                                                                                   netArc,\n",
        "                                                                                   G,\n",
        "                                                                                   app,\n",
        "                                                                                   set_target = False,\n",
        "                                                                                   crop_size=crop_size,\n",
        "                                                                                   BS=batch_size)\n",
        "\n",
        "result = get_final_image(final_frames_list, crop_frames_list, full_frames[0], tfm_array_list, handler)\n",
        "cv2.imwrite('examples/results/result.png', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQaz-nG_hU1h",
        "outputId": "56379f7c-c132-4344-831e-6f95db6fd062"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 62.29it/s]\n",
            "1it [00:00, 1567.96it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 17.37it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 11428.62it/s]\n",
            "/content/sber-swap/utils/inference/masks.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  lmrks = np.array( lmrks.copy(), dtype=np.int )\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**画像を表示**\n",
        "#@markdown ・画像は examples/results/results.png に保存されています\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "show_images([source[0][:, :, ::-1], target_full, result], ['Source Image', 'Target Image', 'Swapped Image'], figsize=(20, 15))"
      ],
      "metadata": {
        "id": "IPc5nm3khC8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**動画をFaceswap**"
      ],
      "metadata": {
        "id": "AmidTUE9oAmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #**動画をFaceswap**\n",
        "#@markdown ・自分の画像は examples/images にアップロードして下さい\\\n",
        "#@markdown ・自分の動画は examples/videos にアップロードして下さい\n",
        "source = 'angelina.jpg' #@param {type:\"string\"}\n",
        "video = 'yui.mp4' #@param {type:\"string\"}\n",
        "source_path = 'examples/images/'+source\n",
        "path_to_video = 'examples/videos/'+video\n",
        "\n",
        "source_full = cv2.imread(source_path)\n",
        "OUT_VIDEO_NAME = \"examples/results/result.mp4\"\n",
        "crop_size = 224 # don't change this\n",
        "batch_size =  40\n",
        "\n",
        "source = crop_face(source_full, app, crop_size)[0]\n",
        "source = [source[:, :, ::-1]]\n",
        "\n",
        "full_frames, fps = read_video(path_to_video)\n",
        "target = get_target(full_frames, app, crop_size)\n",
        "\n",
        "START_TIME = time.time()\n",
        "\n",
        "final_frames_list, crop_frames_list, full_frames, tfm_array_list = model_inference(full_frames,\n",
        "                                                                                   source,\n",
        "                                                                                   target,\n",
        "                                                                                   netArc,\n",
        "                                                                                   G,\n",
        "                                                                                   app,\n",
        "                                                                                   set_target = False,\n",
        "                                                                                   crop_size=crop_size,\n",
        "                                                                                   BS=batch_size)\n",
        "\n",
        "if use_sr:\n",
        "    final_frames_list = face_enhancement(final_frames_list, model)\n",
        "\n",
        "get_final_video(final_frames_list,\n",
        "                crop_frames_list,\n",
        "                full_frames,\n",
        "                tfm_array_list,\n",
        "                OUT_VIDEO_NAME,\n",
        "                fps, \n",
        "                handler)\n",
        "  \n",
        "add_audio_from_another_video(path_to_video, OUT_VIDEO_NAME, \"audio\")\n",
        "\n",
        "print(f'Full pipeline took {time.time() - START_TIME}')\n",
        "print(f\"Video saved with path {OUT_VIDEO_NAME}\")"
      ],
      "metadata": {
        "id": "QIIhCxOKn797"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**動画を表示**\n",
        "#@markdown ・動画は examples/results/results.mp4 に保存されています\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_file = open(OUT_VIDEO_NAME, \"r+b\").read()\n",
        "video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "\n",
        "HTML(f\"\"\"<video width={800} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "metadata": {
        "id": "NsQlEz1Qn-j-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}