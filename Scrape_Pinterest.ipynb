{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBkEl3AoL1+ObH+PSpCsCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Scrape_Pinterest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scrape images from Pinterest**"
      ],
      "metadata": {
        "id": "_q4sU18K8zHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM5hgM0B4iIf",
        "outputId": "20b44719-d9d5-47d5-aebb-eeb6acccc8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PinterestScraper'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Total 13 (delta 0), reused 0 (delta 0), pack-reused 13\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n",
            "/content/PinterestScraper\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/civiliangame/PinterestScraper.git\n",
        "%cd PinterestScraper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chromiumとseleniumをインストール\n",
        "print(\"前処理を開始\")\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "#!pip install -U google-colab\n",
        "#!pip install -U datascience\n",
        "#!pip install -U requests\n",
        "#!pip install -U urllib3\n",
        "!pip install selenium\n",
        "\n",
        "#ライブラリをインポート\n",
        "from selenium import webdriver\n",
        "import time\n",
        "\n",
        "#---------------------------------------------------------------------------------------\n",
        "# 処理開始\n",
        "#---------------------------------------------------------------------------------------\n",
        "# ブラウザをheadlessモード実行\n",
        "print(\"\\nブラウザを設定\")\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "# # サイトにアクセス\n",
        "# print(\"サイトにアクセス開始\")\n",
        "# driver.get(\"https://www.google.com/?hl=ja\")\n",
        "# time.sleep(3)\n",
        "\n",
        "# # driver.find_elements_by_css_selector(\"xxx\") 的な処理を自由に\n",
        "# print(\"サイトのタイトル：\", driver.title)\n",
        "\n",
        "# print(\"\\nお疲れさまです。\\n処理が完了しました。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0-cRQqsxsIc",
        "outputId": "092f0823-fa22-4909-e1d6-bf3d510e22d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "前処理を開始\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,867 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,075 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,302 kB]\n",
            "Fetched 9,797 kB in 5s (2,012 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 5s (19.6 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 68.6 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "\n",
            "ブラウザを設定\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modules**"
      ],
      "metadata": {
        "id": "ZP-dF_Wp6-4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support import ui\n",
        "from EnglishScraper import ScrapingEssentials\n",
        "import threading\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def page_is_loaded(driver):\n",
        "    return driver.find_element(By.TAG_NAME, \"body\") != None\n",
        "\n",
        "def login(driver, username, password):\n",
        "    if driver.current_url != \"https://www.pinterest.com/login/?referrer=home_page\":\n",
        "        driver.get(\"https://www.pinterest.com/login/?referrer=home_page\")\n",
        "    wait = ui.WebDriverWait(driver, 10)\n",
        "    wait.until(page_is_loaded)\n",
        "    email = driver.find_element(By.XPATH, \"//input[@type='email']\")\n",
        "    password = driver.find_element(By.XPATH,\"//input[@type='password']\")\n",
        "    email.send_keys(\"sxh779@case.edu\")\n",
        "    password.send_keys(\"multithreading\")\n",
        "    # driver.find_element_by_xpath(\"//div[@data-reactid='30']\").click()\n",
        "    password.submit()\n",
        "    time.sleep(3)\n",
        "    print(\"Teleport Successful!\")\n",
        "\n",
        "# Finds the detailed product page of each \"pin\" for pinterest\n",
        "# def download_pages(driver, valid_urls):\n",
        "#     list_counter = 0\n",
        "\n",
        "#     # Pinterest happens to change its HTML every once in a while to prevent botting.\n",
        "\n",
        "#     # This should account for all the differences\n",
        "#     # soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "#     # for pinWrapper in soup.find_all(\"div\", {\"class\": \"pinWrapper\"}):\n",
        "#     #     class_name = pinWrapper.get(\"class\")\n",
        "#     #     print(class_name)\n",
        "#     #     if \"_o\" in class_name[0]:\n",
        "#     #         print(class_name)\n",
        "#     #         break\n",
        "#     #\n",
        "#     # #Finds the tags of the HTML and adjusts it\n",
        "#     # name = \" \".join(class_name)\n",
        "#     # print(name)\n",
        "\n",
        "#     # Does this until you have 10000 items or the program has gone on for long enough, meaning that it reached the end of results\n",
        "#     beginning = time.time()\n",
        "#     end = time.time()\n",
        "#     while list_counter < 10000 and beginning - end < 30:\n",
        "#         beginning = time.time()\n",
        "#         # ----------------------------------EDIT THE CODE BELOW------------------------------#\n",
        "#         # Locate all the urls of the detailed pins\n",
        "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "#         # for c in soup.find_all(\"div\", {\"class\": name}):\n",
        "#         for pinLink in soup.find_all(\"div\", {\"class\": \"pinWrapper\"}):\n",
        "#             for a in pinLink.find_all(\"a\"):\n",
        "#                 print(pinLink)\n",
        "#                 url = (\"https://pinterest.com\" + str(a.get(\"href\")))\n",
        "#                 print(url)\n",
        "#                 # Checks and makes sure that the pin isn't there already and that random urls are not invited\n",
        "#                 if len(url) < 60 and url not in valid_urls and \"A\" not in url:\n",
        "#                     # ---------------------------------EDIT THE CODE ABOVE-------------------------------#\n",
        "#                     valid_urls.append(url)\n",
        "#                     print(\"found the detailed page of: \" + str(list_counter))\n",
        "#                     list_counter += 1\n",
        "#                     end = time.time()\n",
        "#                 time.sleep(.15)\n",
        "#                 # Scroll down now\n",
        "#         driver.execute_script(\"window.scrollBy(0,300)\")\n",
        "#     return\n",
        "\n",
        "# Downloads the image files from the img urls\n",
        "# def get_pic(valid_urls, driver):\n",
        "#     print(\"hey\")\n",
        "#     get_pic_counter = 0\n",
        "#     time.sleep(5)\n",
        "#     while (get_pic_counter < len(valid_urls)):\n",
        "#         print(0)\n",
        "#         # Now, we can just type in the URL and pinterest will not block us\n",
        "#         for urls in valid_urls:\n",
        "#             driver.get(urls)\n",
        "#             # Wait until the page is loaded\n",
        "#             if driver.current_url == urls:\n",
        "#                 wait = ui.WebDriverWait(driver, 10)\n",
        "#                 wait.until(page_is_loaded)\n",
        "#                 loaded = True\n",
        "#             print(1)\n",
        "#             # -----------------------------------EDIT THE CODE BELOW IF PINTEREST CHANGES---------------------------#\n",
        "#             # Extract the image url\n",
        "#             soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "#             print(2)\n",
        "#             for mainContainer in soup.find_all(\"div\", {\"class\": \"mainContainer\"}):\n",
        "#                 print(3)\n",
        "#                 for closeupContainer in mainContainer.find_all(\"div\", {\"class\": \"closeupContainer\"}):\n",
        "#                     print(4)\n",
        "#                     # for heightContainer in closeupContainer.find_all(\"div\", {\"class\": \"FlashlightEnabledImage Module\"}):\n",
        "#                     print(5)\n",
        "#                     for img in closeupContainer.find_all(\"img\"):\n",
        "#                         print(6)\n",
        "#                         print(\"hello\")\n",
        "#                         img_link = img.get(\"src\")\n",
        "#                         if \"564\" in img_link:\n",
        "#                             print(\"found the img url of: \" + str(get_pic_counter))\n",
        "#                             get_pic_counter += 1\n",
        "#                             t.download_image(img_link)\n",
        "#                             break\n",
        "\n",
        "def get_pin(keyword, img_num, driver):\n",
        "    \n",
        "    #ページを開く\n",
        "    driver.get(\"https://pinterest.com/search/pins/?q=\" + str(keyword) + \"&rs=typed&term_meta[]=\" + str(\n",
        "              keyword) + \"%7Ctyped\")\n",
        "    #ページ読み込み待ち\n",
        "    wait = ui.WebDriverWait(driver, 10)\n",
        "    wait.until(page_is_loaded)\n",
        "    \n",
        "    all_pin_data = []\n",
        "    while 1:\n",
        "        # scroll\n",
        "        driver1.execute_script(\"window.scrollBy(0,10000)\")\n",
        "        time.sleep(3)\n",
        "        driver1.execute_script(\"window.scrollBy(0,10000)\")\n",
        "        time.sleep(3)\n",
        "\n",
        "        # get the html now\n",
        "        page_source = driver.page_source\n",
        "        page = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # 'GrowthUnauthPinImage' is the class of all the pins\n",
        "        # Here we take divs which have a with href as pin number\n",
        "        pin_data = page.find_all('div', \"Yl- MIw Hb7\")\n",
        "        all_pin_data.extend(pin_data)\n",
        "        temp = all_pin_data\n",
        "        all_pin_data = list(set(all_pin_data)) #リスト内の重複を削除\n",
        "\n",
        "        print(\"duplicate: {}\".format(len(temp) - len(all_pin_data)))\n",
        "        print(\"total: {}\".format(len(all_pin_data)))\n",
        "        print(\"\")\n",
        "\n",
        "        #pin_dataの数がimg_numを超えたら終了\n",
        "        if len(all_pin_data) > img_num:\n",
        "            break\n",
        "\n",
        "    # アドレスのリストを追加\n",
        "    url_list = []\n",
        "    for i in range(len(all_pin_data)):\n",
        "        url_list.append('https://www.pinterest.com' +\n",
        "                    all_pin_data[i].find('a')['href'])\n",
        "        print('https://www.pinterest.com' +\n",
        "                    all_pin_data[i].find('a')['href']) \n",
        "    return url_list\n",
        "\n",
        "\n",
        "\n",
        "def get_url(url, driver):\n",
        "\n",
        "    #ページを開く\n",
        "    driver.get(url)\n",
        "    #ページ読み込み待ち\n",
        "    wait = ui.WebDriverWait(driver, 10)\n",
        "    wait.until(page_is_loaded)\n",
        "    \n",
        "    page_source = driver.page_source\n",
        "    page = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "    #page.select(\"link\")\n",
        "    a = page.find(rel=\"preload\")\n",
        "    url = a.get(\"href\")\n",
        "\n",
        "    return url\n",
        "    \n",
        "\n",
        "#画像のURLを取得（ついでに関連画像も）\n",
        "# def get_related_image(url, img_num, driver):\n",
        "#     #ページを開く\n",
        "#     driver.get(url)\n",
        "#     #ページ読み込み待ち\n",
        "#     wait = ui.WebDriverWait(driver, 10)\n",
        "#     wait.until(page_is_loaded)\n",
        "    \n",
        "\n",
        "#     while 1:\n",
        "#         page_source = driver.page_source\n",
        "\n",
        "#         #htmlの中に\"originals\"という文字列が含まれるものを検索しインデックスを出力\n",
        "#         char = \"originals\"\n",
        "#         idxs = [i.start() for i in re.finditer(char, page_source)]\n",
        "\n",
        "#         #インデックスの左右にあるダブルクオーテーションを検索し、その間の文字列を抽出\n",
        "#         #httpsが含まれる文字列に限定することによりアドレスでないものを除外\n",
        "#         url_list2 = []\n",
        "#         for idx in idxs:\n",
        "#             l = page_source[:idx]\n",
        "#             l = l.rsplit('\"', 1)[1]\n",
        "#             r = page_source[idx:]\n",
        "#             r = r.split('\"',1)[0]\n",
        "#             url = l+r\n",
        "#             if \"https\" in url:\n",
        "#                 print(url)\n",
        "#                 url_list2.append(url)\n",
        "#         url_list2 = list(set(url_list2))\n",
        "\n",
        "#         #url_listの数がimg_numを超えたら終了\n",
        "#         if len(url_list2) > img_num:\n",
        "#             url_list = url_list2[:img_num]\n",
        "#             break\n",
        "#         # scroll\n",
        "#         driver.execute_script(\"window.scrollBy(0,10000)\")\n",
        "#         time.sleep(3)\n",
        "\n",
        "#     return url_list2\n",
        "\n",
        "\n",
        "            # ---------------------------------EDIT THE CODE ABOVE IF PINTEREST CHANGES-----------------------------#\n"
      ],
      "metadata": {
        "id": "IUoRBwv66_DH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = ScrapingEssentials(\"Pinterest\")\n",
        "keyword_list = [\"men anime\"]\n",
        "print(keyword_list)\n",
        "\n",
        "#ChromeDriverをheadlessモードにする\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "\n",
        "driver1 = webdriver.Chrome('chromedriver',options=options)\n",
        "driver2 = webdriver.Chrome('chromedriver',options=options)\n",
        "driver1.implicitly_wait(10) # 秒\n",
        "\n",
        "driver1.get(\"https://www.pinterest.com/login/?referrer=home_page\")\n",
        "driver2.get(\"https://www.pinterest.com/login/?referrer=home_page\")\n",
        "\n",
        "login(driver1, \"e-mail\", \"password\")\n",
        "login(driver2, \"e-mail\", \"password\")\n",
        "\n",
        "# loaded = False\n",
        "# while loaded == False:\n",
        "#     if driver1.current_url != \"https://www.pinterest.com/login/?referrer=home_page\":\n",
        "#         loaded = True\n",
        "\n",
        "#カレントページのURLを取得して表示\n",
        "cur_url = driver1.current_url\n",
        "print(\"current_url: {}\".format(cur_url))\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(index=[], columns=['PID_URLS'])\n",
        "\n",
        "#キーワードが含まれる画像をクリックしたときに表示されるページのリストを取得\n",
        "for keyword in keyword_list:\n",
        "    print(\"start\")\n",
        "    print(keyword)\n",
        "\n",
        "    pin_list = get_pin(keyword, 10, driver1)\n",
        "    print(pin_list)\n",
        "    \n",
        "    k=0\n",
        "    for pin in pin_list:\n",
        "        print(url)\n",
        "        url = get_url(pin, driver2)\n",
        "\n",
        "        df2 = pd.DataFrame(index=[], columns=['PID_URLS'])\n",
        "        df2.loc[0,'PID_URLS'] = url\n",
        "        df = df.append(df2)\n",
        "\n",
        "print(\"done\")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oVgL3EGuzxw2",
        "outputId": "a009e92f-2892-46ad-e7f5-3f3900c1c540"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['men anime']\n",
            "Teleport Successful!\n",
            "Teleport Successful!\n",
            "current_url: https://www.pinterest.com/login/?referrer=home_page\n",
            "start\n",
            "men anime\n",
            "duplicate: 0\n",
            "total: 13\n",
            "\n",
            "https://www.pinterest.com/pin/294282156911860979/\n",
            "https://www.pinterest.com/pin/290060032262847378/\n",
            "https://www.pinterest.com/pin/882635226944908469/\n",
            "https://www.pinterest.com/pin/992621574092726950/\n",
            "https://www.pinterest.com/pin/109775309659165762/\n",
            "https://www.pinterest.com/pin/375206212689746981/\n",
            "https://www.pinterest.com/pin/239676011410233331/\n",
            "https://www.pinterest.com/pin/519673244505793261/\n",
            "https://www.pinterest.com/pin/655484920777324773/\n",
            "https://www.pinterest.com/pin/636274253620999416/\n",
            "https://www.pinterest.com/pin/296111744260578160/\n",
            "https://www.pinterest.com/pin/1104718983567357652/\n",
            "https://www.pinterest.com/pin/633740978935210327/\n",
            "['https://www.pinterest.com/pin/294282156911860979/', 'https://www.pinterest.com/pin/290060032262847378/', 'https://www.pinterest.com/pin/882635226944908469/', 'https://www.pinterest.com/pin/992621574092726950/', 'https://www.pinterest.com/pin/109775309659165762/', 'https://www.pinterest.com/pin/375206212689746981/', 'https://www.pinterest.com/pin/239676011410233331/', 'https://www.pinterest.com/pin/519673244505793261/', 'https://www.pinterest.com/pin/655484920777324773/', 'https://www.pinterest.com/pin/636274253620999416/', 'https://www.pinterest.com/pin/296111744260578160/', 'https://www.pinterest.com/pin/1104718983567357652/', 'https://www.pinterest.com/pin/633740978935210327/']\n",
            "https://i.pinimg.com/originals/ab/da/8b/abda8bb855e721a778820dd9ffeef6d3.jpg\n",
            "https://i.pinimg.com/originals/60/52/f7/6052f7429d6b41428be9556901c63327.jpg\n",
            "https://i.pinimg.com/originals/3f/d3/ac/3fd3ac69c3f1f5b0716ec7ed9ac9fb96.jpg\n",
            "https://i.pinimg.com/originals/07/5c/db/075cdb98212f81108e58ee402725324f.jpg\n",
            "https://i.pinimg.com/originals/55/37/28/55372853866fc3c09a39813eadbc1412.jpg\n",
            "https://i.pinimg.com/originals/07/05/7d/07057d7162c61dbf8f4b88ad3df1b5af.jpg\n",
            "https://i.pinimg.com/originals/13/9b/3e/139b3e192492c39232c3858f2024b758.jpg\n",
            "https://i.pinimg.com/originals/f3/c8/b9/f3c8b9276268ce247abfd16f97508897.jpg\n",
            "https://i.pinimg.com/originals/40/4e/bb/404ebb1054ef23e2e267d888679d4c57.jpg\n",
            "https://i.pinimg.com/originals/cb/ce/a2/cbcea274ce429f6804dee6dc5a0463da.jpg\n",
            "https://i.pinimg.com/originals/b5/70/a0/b570a07447d7ac00112812a7dcc710c6.jpg\n",
            "https://i.pinimg.com/originals/69/cd/a8/69cda80cfd7c6b854d62aa3bb0fa8c63.jpg\n",
            "https://i.pinimg.com/originals/9c/d5/fe/9cd5fef01a4f66b88805d31c90d887c7.jpg\n",
            "done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            PID_URLS\n",
              "0  https://i.pinimg.com/originals/60/52/f7/6052f7...\n",
              "0  https://i.pinimg.com/originals/3f/d3/ac/3fd3ac...\n",
              "0  https://i.pinimg.com/originals/07/5c/db/075cdb...\n",
              "0  https://i.pinimg.com/originals/55/37/28/553728...\n",
              "0  https://i.pinimg.com/originals/07/05/7d/07057d...\n",
              "0  https://i.pinimg.com/originals/13/9b/3e/139b3e...\n",
              "0  https://i.pinimg.com/originals/f3/c8/b9/f3c8b9...\n",
              "0  https://i.pinimg.com/originals/40/4e/bb/404ebb...\n",
              "0  https://i.pinimg.com/originals/cb/ce/a2/cbcea2...\n",
              "0  https://i.pinimg.com/originals/b5/70/a0/b570a0...\n",
              "0  https://i.pinimg.com/originals/69/cd/a8/69cda8...\n",
              "0  https://i.pinimg.com/originals/9c/d5/fe/9cd5fe...\n",
              "0  https://i.pinimg.com/originals/8b/c1/1b/8bc11b..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1754fea5-7775-41c4-9f7e-6d7f30d8a119\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID_URLS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/60/52/f7/6052f7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/3f/d3/ac/3fd3ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/07/5c/db/075cdb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/55/37/28/553728...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/07/05/7d/07057d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/13/9b/3e/139b3e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/f3/c8/b9/f3c8b9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/40/4e/bb/404ebb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/cb/ce/a2/cbcea2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/b5/70/a0/b570a0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/69/cd/a8/69cda8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/9c/d5/fe/9cd5fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://i.pinimg.com/originals/8b/c1/1b/8bc11b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1754fea5-7775-41c4-9f7e-6d7f30d8a119')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1754fea5-7775-41c4-9f7e-6d7f30d8a119 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1754fea5-7775-41c4-9f7e-6d7f30d8a119');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**動きの確認**"
      ],
      "metadata": {
        "id": "C-5JpBzmBxYL"
      }
    }
  ]
}