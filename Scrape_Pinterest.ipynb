{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbdEdL0Stcgl+huic+ew7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Scrape_Pinterest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scrape images from Pinterest**"
      ],
      "metadata": {
        "id": "_q4sU18K8zHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM5hgM0B4iIf",
        "outputId": "c5faefb3-e545-4a8f-b938-1b157f4ac97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PinterestScraper'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Total 13 (delta 0), reused 0 (delta 0), pack-reused 13\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n",
            "/content/PinterestScraper\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/civiliangame/PinterestScraper.git\n",
        "%cd PinterestScraper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chromiumとseleniumをインストール\n",
        "print(\"前処理を開始\")\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "#!pip install -U google-colab\n",
        "#!pip install -U datascience\n",
        "#!pip install -U requests\n",
        "#!pip install -U urllib3\n",
        "!pip install selenium\n",
        "\n",
        "#ライブラリをインポート\n",
        "from selenium import webdriver\n",
        "import time\n",
        "\n",
        "#---------------------------------------------------------------------------------------\n",
        "# 処理開始\n",
        "#---------------------------------------------------------------------------------------\n",
        "# ブラウザをheadlessモード実行\n",
        "print(\"\\nブラウザを設定\")\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "# # サイトにアクセス\n",
        "# print(\"サイトにアクセス開始\")\n",
        "# driver.get(\"https://www.google.com/?hl=ja\")\n",
        "# time.sleep(3)\n",
        "\n",
        "# # driver.find_elements_by_css_selector(\"xxx\") 的な処理を自由に\n",
        "# print(\"サイトのタイトル：\", driver.title)\n",
        "\n",
        "# print(\"\\nお疲れさまです。\\n処理が完了しました。\")"
      ],
      "metadata": {
        "id": "r0-cRQqsxsIc",
        "outputId": "2ecbf0da-86af-41cb-9bf7-bacf93b44d59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "前処理を開始\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,867 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,075 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,302 kB]\n",
            "Fetched 9,797 kB in 2s (5,052 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 1s (70.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 16.9 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 92.7 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 89.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 62.7 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "\n",
            "ブラウザを設定\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modules**"
      ],
      "metadata": {
        "id": "ZP-dF_Wp6-4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support import ui\n",
        "from EnglishScraper import ScrapingEssentials\n",
        "import threading\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "\n",
        "def page_is_loaded(driver):\n",
        "    return driver.find_element(By.TAG_NAME, \"body\") != None\n",
        "\n",
        "def login(driver, username, password):\n",
        "    if driver.current_url != \"https://www.pinterest.com/login/?referrer=home_page\":\n",
        "        driver.get(\"https://www.pinterest.com/login/?referrer=home_page\")\n",
        "    wait = ui.WebDriverWait(driver, 10)\n",
        "    wait.until(page_is_loaded)\n",
        "    email = driver.find_element(By.XPATH, \"//input[@type='email']\")\n",
        "    password = driver.find_element(By.XPATH,\"//input[@type='password']\")\n",
        "    email.send_keys(\"sxh779@case.edu\")\n",
        "    password.send_keys(\"multithreading\")\n",
        "    # driver.find_element_by_xpath(\"//div[@data-reactid='30']\").click()\n",
        "    password.submit()\n",
        "    time.sleep(3)\n",
        "    print(\"Teleport Successful!\")\n",
        "\n",
        "# Finds the detailed product page of each \"pin\" for pinterest\n",
        "def download_pages(driver, valid_urls):\n",
        "    list_counter = 0\n",
        "\n",
        "    # Pinterest happens to change its HTML every once in a while to prevent botting.\n",
        "\n",
        "    # This should account for all the differences\n",
        "    # soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "    # for pinWrapper in soup.find_all(\"div\", {\"class\": \"pinWrapper\"}):\n",
        "    #     class_name = pinWrapper.get(\"class\")\n",
        "    #     print(class_name)\n",
        "    #     if \"_o\" in class_name[0]:\n",
        "    #         print(class_name)\n",
        "    #         break\n",
        "    #\n",
        "    # #Finds the tags of the HTML and adjusts it\n",
        "    # name = \" \".join(class_name)\n",
        "    # print(name)\n",
        "\n",
        "    # Does this until you have 10000 items or the program has gone on for long enough, meaning that it reached the end of results\n",
        "    beginning = time.time()\n",
        "    end = time.time()\n",
        "    while list_counter < 10000 and beginning - end < 30:\n",
        "        beginning = time.time()\n",
        "        # ----------------------------------EDIT THE CODE BELOW------------------------------#\n",
        "        # Locate all the urls of the detailed pins\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        # for c in soup.find_all(\"div\", {\"class\": name}):\n",
        "        for pinLink in soup.find_all(\"div\", {\"class\": \"pinWrapper\"}):\n",
        "            for a in pinLink.find_all(\"a\"):\n",
        "                print(pinLink)\n",
        "                url = (\"https://pinterest.com\" + str(a.get(\"href\")))\n",
        "                print(url)\n",
        "                # Checks and makes sure that the pin isn't there already and that random urls are not invited\n",
        "                if len(url) < 60 and url not in valid_urls and \"A\" not in url:\n",
        "                    # ---------------------------------EDIT THE CODE ABOVE-------------------------------#\n",
        "                    valid_urls.append(url)\n",
        "                    print(\"found the detailed page of: \" + str(list_counter))\n",
        "                    list_counter += 1\n",
        "                    end = time.time()\n",
        "                time.sleep(.15)\n",
        "                # Scroll down now\n",
        "        driver.execute_script(\"window.scrollBy(0,300)\")\n",
        "    return\n",
        "\n",
        "# Downloads the image files from the img urls\n",
        "def get_pic(valid_urls, driver):\n",
        "    print(\"hey\")\n",
        "    get_pic_counter = 0\n",
        "    time.sleep(5)\n",
        "    while (get_pic_counter < len(valid_urls)):\n",
        "        print(0)\n",
        "        # Now, we can just type in the URL and pinterest will not block us\n",
        "        for urls in valid_urls:\n",
        "            driver.get(urls)\n",
        "            # Wait until the page is loaded\n",
        "            if driver.current_url == urls:\n",
        "                wait = ui.WebDriverWait(driver, 10)\n",
        "                wait.until(page_is_loaded)\n",
        "                loaded = True\n",
        "            print(1)\n",
        "            # -----------------------------------EDIT THE CODE BELOW IF PINTEREST CHANGES---------------------------#\n",
        "            # Extract the image url\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "            print(2)\n",
        "            for mainContainer in soup.find_all(\"div\", {\"class\": \"mainContainer\"}):\n",
        "                print(3)\n",
        "                for closeupContainer in mainContainer.find_all(\"div\", {\"class\": \"closeupContainer\"}):\n",
        "                    print(4)\n",
        "                    # for heightContainer in closeupContainer.find_all(\"div\", {\"class\": \"FlashlightEnabledImage Module\"}):\n",
        "                    print(5)\n",
        "                    for img in closeupContainer.find_all(\"img\"):\n",
        "                        print(6)\n",
        "                        print(\"hello\")\n",
        "                        img_link = img.get(\"src\")\n",
        "                        if \"564\" in img_link:\n",
        "                            print(\"found the img url of: \" + str(get_pic_counter))\n",
        "                            get_pic_counter += 1\n",
        "                            t.download_image(img_link)\n",
        "                            break\n",
        "\n",
        "            # ---------------------------------EDIT THE CODE ABOVE IF PINTEREST CHANGES-----------------------------#\n"
      ],
      "metadata": {
        "id": "IUoRBwv66_DH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = ScrapingEssentials(\"Pinterest\")\n",
        "e_list = t.english_pickle()\n",
        "print(e_list)\n",
        "\n",
        "#ChromeDriverをheadlessモードにする\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "\n",
        "driver1 = webdriver.Chrome('chromedriver',options=options)\n",
        "driver2 = webdriver.Chrome('chromedriver',options=options)\n",
        "driver1.implicitly_wait(10) # 秒\n",
        "\n",
        "driver1.get(\"https://www.pinterest.com/login/?referrer=home_page\")\n",
        "driver2.get(\"https://www.pinterest.com/login/?referrer=home_page\")\n",
        "\n",
        "login(driver1, \"e-mail\", \"password\")\n",
        "login(driver2, \"e-mail\", \"password\")\n",
        "\n",
        "# loaded = False\n",
        "# while loaded == False:\n",
        "#     if driver1.current_url != \"https://www.pinterest.com/login/?referrer=home_page\":\n",
        "#         loaded = True\n",
        "\n",
        "#カレントページのURLを取得して表示\n",
        "cur_url = driver1.current_url\n",
        "print(\"current_url: {}\".format(cur_url))\n",
        "\n",
        "\n",
        "for item in e_list:\n",
        "        print(\"start\")\n",
        "        keyword = item\n",
        "        valid_urls = []\n",
        "        print(keyword)\n",
        "        driver1.get(\"https://pinterest.com/search/pins/?q=\" + str(keyword) + \"&rs=typed&term_meta[]=\" + str(\n",
        "            keyword) + \"%7Ctyped\")\n",
        "        \n",
        "        print(valid_urls)\n",
        "\n",
        "        time.sleep(3)\n",
        "        t1 = threading.Thread(target=download_pages, args=(driver1, valid_urls,))\n",
        "        t1.setDaemon(True)\n",
        "        t1.start()\n",
        "\n",
        "        t2 = threading.Thread(target=get_pic, args=(valid_urls, driver2,))\n",
        "        t2.setDaemon(True)\n",
        "        t2.start()\n",
        "        #\n",
        "        t1.join()\n",
        "        # t2.join()\n",
        "\n",
        "        t.reset()\n",
        "        print(\"done\")"
      ],
      "metadata": {
        "id": "oVgL3EGuzxw2",
        "outputId": "f8f940da-6524-4b37-a7b2-7dce4f524f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hyundai', 'Kia']\n",
            "Teleport Successful!\n",
            "Teleport Successful!\n",
            "current_url: https://www.pinterest.com/login/?referrer=home_page\n",
            "start\n",
            "Hyundai\n",
            "[]\n",
            "hey\n",
            "done\n",
            "start\n",
            "Kia\n",
            "[]\n",
            "hey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**動きの確認**"
      ],
      "metadata": {
        "id": "C-5JpBzmBxYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(keyword)"
      ],
      "metadata": {
        "id": "WDScRIOHBqrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#アドレスが開くかどうかを確認\n",
        "driver1.get(\"https://pinterest.com/search/pins/?q=\" + str(keyword) + \"&rs=typed&term_meta[]=\" + str(\n",
        "            keyword) + \"%7Ctyped\")\n"
      ],
      "metadata": {
        "id": "Rch7fVeS77RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#カレントページのURLを取得して表示\n",
        "cur_url = driver1.current_url\n",
        "print(\"current_url: {}\".format(cur_url))"
      ],
      "metadata": {
        "id": "7Dr-rzT6Bhz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver1.execute_script(\"window.scrollBy(0,10000)\")\n",
        "time.sleep(3)\n",
        "driver1.execute_script(\"window.scrollBy(0,10000)\")\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "fGllqRg7LKTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(driver1.page_source, \"html.parser\")\n",
        "print(soup)"
      ],
      "metadata": {
        "id": "Im6iPJpBBYiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pin_data = []\n",
        "\n",
        "while 1:\n",
        "    # scroll\n",
        "    driver1.execute_script(\"window.scrollBy(0,10000)\")\n",
        "    time.sleep(3)\n",
        "    driver1.execute_script(\"window.scrollBy(0,10000)\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    # get the html now\n",
        "    page_source = driver1.page_source\n",
        "    page = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "    # 'GrowthUnauthPinImage' is the class of all the pins\n",
        "    # Here we take divs which have a with href as pin number\n",
        "    pin_data = page.find_all('div', \"Yl- MIw Hb7\")\n",
        "    print(len(pin_data))\n",
        "    all_pin_data.extend(pin_data)\n",
        "    all_pin_data = list(set(all_pin_data)) #リスト内の重複を削除\n",
        "\n",
        "    if len(all_pin_data) > 50:\n",
        "        break"
      ],
      "metadata": {
        "id": "rwCES9I_LX5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get links for individual pages of all the Pins\n",
        "hrefs = []\n",
        "for i in range(len(all_pin_data)):\n",
        "    hrefs.append('https://www.pinterest.com' +\n",
        "                 all_pin_data[i].find('a')['href'] + 'visual-search/')\n",
        "\n",
        "# save to CSV\n",
        "df = pd.DataFrame({'PID_URLS': hrefs[:50]})\n",
        "df.to_csv(csv_name, index=False)"
      ],
      "metadata": {
        "id": "0dlJFDagOFSx",
        "outputId": "e3439f9f-f2a5-48ef-cca2-b802f9ebf86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-44334bccffab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pin_data[0])"
      ],
      "metadata": {
        "id": "r6hDElEzNHCX",
        "outputId": "a335c539-1a1c-4b4e-ce59-f3f1f0b557bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div class=\"Yl- MIw Hb7\" data-grid-item=\"true\" role=\"listitem\" style=\"top: 0px; left: 0px; transform: translateX(26px) translateY(24951px); width: 236px; height: 179px;\"><div aria-disabled=\"false\" class=\"CCY czT eEj iyn DI9 BG7\" role=\"button\" tabindex=\"0\"><div class=\"zI7 iyn Hsu\"><div class=\"zI7 iyn Hsu zmN\" data-test-id=\"pin\" data-test-pin-id=\"364299057366051677\"><div class=\"lnZ wsz zI7 iyn Hsu zmN\" data-test-id=\"pin-with-alt-text\"><div class=\"zI7 iyn Hsu\" data-test-id=\"deeplink-wrapper\"><a class=\"Wk9 CCY czT ho- kVc xQ4 iyn\" href=\"/pin/364299057366051677/\" rel=\"\"><div class=\"PinCard__imageWrapper\" style=\"position: relative;\"><div class=\"XiG ho- sLG zI7 iyn Hsu\" data-test-id=\"pincard-image-with-link\" style=\"will-change: transform; text-decoration: none;\"><div class=\"XiG zI7 iyn Hsu\" data-test-id=\"pin-visual-wrapper\" style=\"margin-top: 0%; margin-bottom: 0%;\"><div class=\"Pj7 sLG XiG ho- m1e\"><div class=\"XiG zI7 iyn Hsu\" style=\"background-color: rgb(137, 145, 143); padding-bottom: 58.8983%;\"><img alt=\"2014 - 2015 Kia Optima | car review @ Top Speed\" class=\"hCL kVc L4E MIw\" importance=\"auto\" loading=\"auto\" src=\"https://i.pinimg.com/236x/a9/33/98/a93398bb89d5053baff87322d8201162--kia-optima-turbo-optima-car.jpg\"/></div><div class=\"KPc MIw ojN Rym p6V QLY\"></div></div></div></div></div></a></div><div class=\"zI7 iyn Hsu\"><div class=\"zI7 iyn Hsu\" data-test-id=\"deeplink-wrapper\"><a class=\"Wk9 CCY czT ho- kVc xQ4 iyn\" href=\"/pin/364299057366051677/\" rel=\"\"><div class=\"Jea Rz6 XiG xvE zI7 iyn Hsu\"><div class=\"ujU zI7 iyn Hsu\"><div class=\"Jea hDW zI7 iyn Hsu\"><div class=\"ujU zI7 iyn Hsu\"><div class=\"hDW sLG zI7 iyn Hsu\" style=\"display: -webkit-box; text-overflow: ellipsis; -webkit-box-orient: vertical; -webkit-line-clamp: 2;\"></div><div class=\"hDW sLG zI7 iyn Hsu\" data-test-id=\"desc\" style=\"display: -webkit-box; text-overflow: ellipsis; -webkit-box-orient: vertical; -webkit-line-clamp: 2;\"><div aria-disabled=\"false\" class=\"CCY czT eEj iyn FTD L4E DI9 BG7\" role=\"button\" tabindex=\"0\"><span class=\"tBJ dyH iFc dR0 O2T zDA IZT swG\">2014 - 2015 Kia Optima | car review @ Top Speed</span></div></div></div></div></div><div class=\"p6V zI7 iyn Hsu\"></div></div></a></div></div></div></div></div></div></div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "a = soup.find('div')\n",
        "print(a)"
      ],
      "metadata": {
        "id": "A6l9HKdMD_KZ",
        "outputId": "fd16c37c-ff8a-41b4-aa7c-1a670059d6b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div id=\"dpm_images\"><img height=\"1\" id=\"dpm_pixel_unauth\" src=\"https://p.tvpixel.com/i?aid=pinterest-aac27acd-90e7-48ad-9f38-60b7bf197c22&amp;e=pv&amp;p=web&amp;tv=1x1&amp;url=https%3A%2F%2Fwww.pinterest.com%2Fsearch%2Fpins%2F%3Fq%3DKia%26rs%3Dtyped%26term_meta%5B%5D%3DKia%257Ctyped&amp;refr=&amp;uid=7cb2e8f77bad41149a4cb429ccf72bff\" style=\"display: none;\" width=\"1\"/></div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for mainContainer in soup.find_all(\"div\", {\"class\": \"mainContainer\"}):\n",
        "    print(3)\n",
        "    for closeupContainer in mainContainer.find_all(\"div\", {\"class\": \"closeupContainer\"}):\n",
        "        print(4)\n",
        "        # for heightContainer in closeupContainer.find_all(\"div\", {\"class\": \"FlashlightEnabledImage Module\"}):\n",
        "        print(5)\n",
        "        for img in closeupContainer.find_all(\"img\"):\n",
        "            print(6)\n",
        "            print(\"hello\")\n",
        "            img_link = img.get(\"src\")\n",
        "            if \"564\" in img_link:\n",
        "                print(\"found the img url of: \" + str(get_pic_counter))\n",
        "                get_pic_counter += 1\n",
        "                t.download_image(img_link)\n",
        "                break"
      ],
      "metadata": {
        "id": "z09WU-UMBNB6"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}