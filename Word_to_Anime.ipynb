{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CLIP + TADNE (pytorch) v2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/Word_to_Anime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Word_to_Anime**\n",
        "\n",
        "This waifu does not exist: https://www.thiswaifudoesnotexist.net/\n",
        "This anime does not exist: https://thisanimedoesnotexist.ai/\n"
      ],
      "metadata": {
        "id": "jVphmMZVh88j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJO_WEB2NHeF"
      },
      "source": [
        "#Import TADNE and convert to pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0yGj-qDOTcY"
      },
      "source": [
        "##readme\n",
        "\n",
        "This notebook was created by Logan Zoellner (@nagolinc, https://loganzoellner.com)\n",
        "\n",
        "\n",
        "This notebook makes use of @AydaoAI's \"This Anime Does not exist\" (which you can read more about here: https://www.gwern.net/Faces#extended-stylegan2-danbooru2019-aydao) and CLIP (which you can read more about here https://github.com/openai/CLIP).\n",
        "\n",
        "Much of it was based off of this notebook: https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb created by @openai\n",
        "\n",
        "And this notebook: https://colab.research.google.com/drive/1oxcJ1tbG77hlggdKd_d8h22nBcIZsLTL by @arfa\n",
        "\n",
        "TADNE is under a CC BY-NC liscence and CLIP is under the MIT Liscence.\n",
        "\n",
        "Any code in this notebook not otherwise liscenced is herein released under the MIT Liscence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ydWNttkMh542"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixs37iA-Mxf2",
        "outputId": "b18550eb-8a11-40bb-eac7-cfeaaec877dc"
      },
      "source": [
        "!git clone https://github.com/shawwn/stylegan2 -b estimator /content/stylegan2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/stylegan2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uqtRVenylum",
        "outputId": "e546f759-8e9f-425e-856b-af4934fac655"
      },
      "source": [
        "cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXcjXwwhzJOW",
        "outputId": "b8a6818b-21a6-4f08-e92d-7256a8f0f253"
      },
      "source": [
        "import gdown, os\n",
        "\n",
        "#モデルをダウンロード\n",
        "if not os.path.exists(\"/content/network-tadne.pkl\"):\n",
        "  #gdown.download('https://drive.google.com/uc?id=1qNhyusI0hwBLI-HOavkNP5I0J0-kcN4C', 'network-tadne.pkl', quiet=False)\n",
        "  url='https://drive.google.com/uc?id=1LCkyOPmcWBsPlQX_DxKAuPM1Ew_nh83I'\n",
        "  gdown.download(url, '/content/network-tadne.pkl', quiet=False)\n",
        "\n",
        "!ls -lrt /content/network-tadne.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1056544230 Jul  2 00:09 /content/network-tadne.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n",
        "%cd /content/stylegan2 #ここでランタイムの再起動を求められる"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iANtwFOpQgP-",
        "outputId": "2564de24-460a-4507-c7a7-8131804711e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "/content/stylegan2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt4kvzEM39oI",
        "outputId": "f1bd4352-bf3b-4f15-cd18-0b7d8159db39"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import scipy\n",
        "\n",
        "#モデルをロードする\n",
        "tflib.init_tf()\n",
        "_G, _D, Gs = pickle.load(open(\"/content/network-tadne.pkl\", \"rb\"))\n",
        "# _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
        "# _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
        "# Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Compiling... Loading... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HSwACJLNwnT",
        "outputId": "aac26b78-c789-43b8-9183-020c4aef0532"
      },
      "source": [
        "!git clone https://github.com/nagolinc/stylegan2-pytorch.git /content/stylegan2-pytorch/\n",
        "!cd /content/stylegan2-pytorch/\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/stylegan2-pytorch'...\n",
            "remote: Enumerating objects: 380, done.\u001b[K\n",
            "remote: Total 380 (delta 0), reused 0 (delta 0), pack-reused 380\u001b[K\n",
            "Receiving objects: 100% (380/380), 122.50 MiB | 27.84 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7YDdobsNwnU"
      },
      "source": [
        "import subprocess\n",
        "\n",
        "#CUDAのセットアップ\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install ninja\n",
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TensorflowのモデルをPytorch用に変換**"
      ],
      "metadata": {
        "id": "3lAqaJfDSaiW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDuwin1PNwnY",
        "outputId": "c808de00-35b4-4dee-ddd2-f7fded414ca6"
      },
      "source": [
        "%cd /content/stylegan2-pytorch\n",
        "from convert_weight import convertStyleGan2\n",
        "\n",
        "#tfのモデルをPytorch用に変換\n",
        "#ckpt: weight\n",
        "#g: generatorのmodel\n",
        "#disc: discriminatorのmodel\n",
        "#g-train: long term average of generator\n",
        "ckpt, g, disc,g_train = convertStyleGan2(_G,_D,Gs)\n",
        "latent_avg=ckpt[\"latent_avg\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42OIJbxoO9IZ"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#出力されたテンソルをPIL形式で表示できるように変換\n",
        "def fmtImg(r):\n",
        "    img = ((r+1)/2*256).clip(0,255).astype(np.uint8).transpose(1,2,0)\n",
        "    return PIL.Image.fromarray(img, 'RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOQ8wwkeA5jG"
      },
      "source": [
        "device='cuda'\n",
        "n_sample=1\n",
        "\n",
        "g = g.to(device)\n",
        "\n",
        "inputSize=1024\n",
        "\n",
        "#[1,1024]の行列を作成\n",
        "z = np.random.RandomState(1).randn(n_sample, inputSize).astype(\"float32\")\n",
        "\n",
        "##################\n",
        "##Pytorchによる生成\n",
        "##################\n",
        "with torch.no_grad():\n",
        "    img_pt, _ = g(\n",
        "        [torch.from_numpy(z).to(device)],\n",
        "        truncation=0.5,\n",
        "        truncation_latent=latent_avg.to(device),\n",
        "        randomize_noise=False,\n",
        "    )\n",
        "\n",
        "###################\n",
        "## Tensorflowによる生成\n",
        "###################\n",
        "Gs_kwargs = dnnlib.EasyDict()\n",
        "Gs_kwargs.randomize_noise = False\n",
        "#img_tf = g_ema.run(z, None, **Gs_kwargs)\n",
        "img_tf = Gs.run(z, None, **Gs_kwargs)\n",
        "img_tf = torch.from_numpy(img_tf).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdm0RUj2FqqE"
      },
      "source": [
        "#verify that these images match\n",
        "#TensorflowとPytorchによる生成画像が一致することを確認\n",
        "img=img_pt.cpu().numpy()[0]\n",
        "display(fmtImg(img))\n",
        "img = img_tf.cpu().numpy()[0]\n",
        "display(fmtImg(img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPRfDBaPQjRv",
        "outputId": "2330b153-1274-4555-8489-b68860a2733a"
      },
      "source": [
        "#Dicriminatorによるアウトプットも一致することを確認\n",
        "device='cuda'\n",
        "\n",
        "img=img_tf.cpu().numpy()\n",
        "score0=_D.run(img,None)\n",
        "disc.to(device)\n",
        "with torch.no_grad():\n",
        "    #numpy to tensor\n",
        "    score1 = disc(\n",
        "          torch.from_numpy(img).to(device)\n",
        "    )\n",
        "#verify that these scores match\n",
        "print(score0,score1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.527499]] tensor([[-1.5275]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPDpVHmAaTO3"
      },
      "source": [
        "#Import CLIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cboKZocQlSYX",
        "outputId": "efb2c813-e95b-4d5a-df52-4eb03a2b3d1b"
      },
      "source": [
        "#MODELのダウンロード\n",
        "MODELS = {\n",
        "    \"ViT-B/32\":       \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "}\n",
        "\n",
        "! wget {MODELS[\"ViT-B/32\"]} -O model.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-02 01:09:37--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.67, 13.107.213.67, 2620:1ec:bdf::67, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353976522 (338M) [application/octet-stream]\n",
            "Saving to: ‘model.pt’\n",
            "\n",
            "model.pt            100%[===================>] 337.58M   110MB/s    in 3.1s    \n",
            "\n",
            "2022-07-02 01:09:40 (110 MB/s) - ‘model.pt’ saved [353976522/353976522]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRVTY9lbGm8",
        "outputId": "ef05687a-23d6-4516-e970-a8740e3ca84a"
      },
      "source": [
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 151,277,313\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "## Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6cpiIFHp9N6"
      },
      "source": [
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer. The tokenizer code is hidden in the second cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGom156-i2kL",
        "outputId": "3b80b476-dad0-4208-8cc9-24e426ab745c"
      },
      "source": [
        "#語彙のモデルをダウンロード\n",
        "! pip install ftfy regex\n",
        "! wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "--2022-07-02 01:12:40--  https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.67, 13.107.213.67, 2620:1ec:bdf::67, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  2.37MB/s    in 0.5s    \n",
            "\n",
            "2022-07-02 01:12:41 (2.37 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toGtcd-Ji_MD"
      },
      "source": [
        "#@title\n",
        "# modules\n",
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niP3nnXzZvni"
      },
      "source": [
        "#Gradient Descent\n",
        "We calculate a score based off of:\n",
        "1. Clip score for the description string\n",
        "2. Discriminator score from TADNE\n",
        "3. Regularize, since TADNE expects inputs that are unit normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZWf42apaqc5"
      },
      "source": [
        "from tqdm.notebook import tqdm, trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6YBIpbRa1A4"
      },
      "source": [
        "_preprocess = torch.nn.Sequential(\n",
        "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNyNybPXZz1X"
      },
      "source": [
        "def getScoringFunction(description,phi=1.0,regularization=-0.01/1024,discriminatorWeight=0.001):\n",
        "  #preprocess text\n",
        "  texts=[description]  \n",
        "  tokenizer = SimpleTokenizer()\n",
        "  text_tokens = [tokenizer.encode(\"This is \" + desc) for desc in texts]\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "      text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "  text_input = text_input.cuda()\n",
        "\n",
        "  #this function takes a (latent) as input and returns a PIL image\n",
        "  # latents can be generated using \n",
        "  # latent = np.random.RandomState(seed).randn(1, 1024)\n",
        "  def displayLatent(latent,phi=phi):\n",
        "    with torch.no_grad():\n",
        "      img_pt, _ = g(\n",
        "          [torch.from_numpy(latent).to(device)],\n",
        "          truncation=phi,\n",
        "          truncation_latent=latent_avg.to(device),\n",
        "          randomize_noise=False,\n",
        "      )\n",
        "    img=img_pt.cpu().numpy()[0]\n",
        "    return fmtImg(img)\n",
        "  \n",
        "  #this function takes a latent and returns a score and a derivative\n",
        "  def scoreLatent(latent,phi=phi,regularization=regularization,discriminatorWeight=discriminatorWeight):\n",
        "    with torch.no_grad():\n",
        "      _latent=torch.from_numpy(latent).to(device)\n",
        "      #compute img array from latent\n",
        "      img_pt, _ = g(\n",
        "          [_latent],\n",
        "          truncation=phi,\n",
        "          truncation_latent=latent_avg.to(device),\n",
        "          randomize_noise=False,\n",
        "      )\n",
        "      #pass through CLIP\n",
        "      image_input = _preprocess(img_pt)\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      text_features = model.encode_text(text_input).float()    \n",
        "      #cosine\n",
        "      image_features_n= image_features/image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features_n=text_features/text_features.norm(dim=-1, keepdim=True)\n",
        "      similarity = text_features_n @ image_features_n.T\n",
        "      #clip score is just the similarity\n",
        "      clipScore=similarity.sum()\n",
        "      #discriminator\n",
        "      discScore=disc(img_pt)\n",
        "      #regularization\n",
        "      l2reg=(_latent**2).sum()\n",
        "      loss=l2reg*regularization-discScore*discriminatorWeight-clipScore\n",
        "    score=loss.cpu().numpy().flatten()[0][0]\n",
        "    return score\n",
        "\n",
        "  def scoreLatent3(latent,phi=phi,regularization=regularization,discriminatorWeight=discriminatorWeight):\n",
        "    with torch.no_grad():\n",
        "      _latent=torch.from_numpy(latent).to(device)\n",
        "      #compute img array from latent\n",
        "      img_pt, _ = g(\n",
        "          [_latent],\n",
        "          truncation=phi,\n",
        "          truncation_latent=latent_avg.to(device),\n",
        "          randomize_noise=False,\n",
        "      )\n",
        "      #pass through CLIP\n",
        "      image_input = _preprocess(img_pt)\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      text_features = model.encode_text(text_input).float()    \n",
        "      #cosine\n",
        "      image_features_n= image_features/image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features_n=text_features/text_features.norm(dim=-1, keepdim=True)\n",
        "      similarity = text_features_n @ image_features_n.T\n",
        "      #clip score is just the similarity\n",
        "      clipScore=similarity.sum()\n",
        "      #discriminator\n",
        "      discScore=disc(img_pt)\n",
        "      #regularization\n",
        "      l2reg=(_latent**2).sum()\n",
        "      loss=l2reg*regularization-discScore*discriminatorWeight-clipScore\n",
        "    return clipScore.cpu().item(),discScore.cpu().numpy()[0][0],l2reg.cpu().item(),loss.cpu().item()\n",
        "\n",
        "  #this function takes a latent and returns a score and a derivative\n",
        "  def gradientDescent(latent,nSteps,phi=phi,regularization=regularization,discriminatorWeight=discriminatorWeight,lr=0.01):\n",
        "\n",
        "    _latent=torch.from_numpy(latent).to(device)\n",
        "    _latent.requires_grad=True\n",
        "\n",
        "    #optim=torch.optim.Adam([_latent], lr=lr)\n",
        "    #optim=torch.optim.SGD([_latent], lr=lr)\n",
        "    optim=torch.optim.AdamW([_latent])\n",
        "\n",
        "    for step in range(nSteps):\n",
        "      #clear gradient\n",
        "      optim.zero_grad()\n",
        "      #compute img array from latent\n",
        "      img_pt, _ = g(\n",
        "          [_latent],\n",
        "          truncation=phi,\n",
        "          truncation_latent=latent_avg.to(device),\n",
        "          randomize_noise=False,\n",
        "      )\n",
        "      #send through CLIP\n",
        "      image_input = _preprocess(img_pt)\n",
        "      image_features = model.encode_image(image_input).float()\n",
        "      text_features = model.encode_text(text_input).float()      \n",
        "      #compute CosDistance\n",
        "      image_features_n= image_features/image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features_n=text_features/text_features.norm(dim=-1, keepdim=True)\n",
        "      similarity = text_features_n @ image_features_n.T\n",
        "      #appy loss\n",
        "      #clip score is just the similarity\n",
        "      clipScore=similarity.sum()\n",
        "      #discriminator\n",
        "      discScore=disc(img_pt)\n",
        "      #regularization\n",
        "      l2reg=(_latent**2).sum()\n",
        "      loss=l2reg*regularization-discScore*discriminatorWeight-clipScore\n",
        "      #compute gradient\n",
        "      loss.backward()\n",
        "      #step optimizer\n",
        "      if step<nSteps-1:\n",
        "        optim.step()\n",
        "    score=loss.detach().cpu().numpy()[0][0]\n",
        "    \n",
        "    return score,_latent.detach().cpu().numpy()\n",
        "\n",
        "  return scoreLatent3,displayLatent,gradientDescent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjWRPTVLaIah"
      },
      "source": [
        "def searchAndClimb(description,n,k,t,phi=1.0,regularization=0.005/1024,discriminatorWeight=0.001,show=False):\n",
        "  '''\n",
        "  First generate n examples and score them\n",
        "   then take the top k scoring examples and run t steps of hill-climbing\n",
        "  '''\n",
        "  #generate scoring function\n",
        "  scoreLatent3,displayLatent,gradientDescent=getScoringFunction(description,phi=1.0,regularization=0.005/1024,discriminatorWeight=0.001)\n",
        "  #generate n results and take the top k\n",
        "  latents=[]\n",
        "  scores=[]\n",
        "  for i in trange(n):\n",
        "    latent = np.random.RandomState().randn(1, 1024).astype(\"float32\")\n",
        "    score=scoreLatent3(latent)[-1]\n",
        "    scores+=[score]\n",
        "    latents+=[latent]\n",
        "  topK=np.argsort(scores)[:k]\n",
        "  #run hill climbing t steps on winning\n",
        "  results=[]\n",
        "  for i in trange(k):\n",
        "    _score,_result=gradientDescent(latents[topK[i]],t,lr=0.07)\n",
        "    results+=[(_result,_score)]\n",
        "    if show:\n",
        "      print(i,scores[topK[i]],scoreLatent3(_result))\n",
        "      display(displayLatent(_result))\n",
        "  return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82cEFpdNbdj_"
      },
      "source": [
        "#and now we can finally do the search\n",
        "The main variables to tweak are:\n",
        "0. description, the string to search for\n",
        "1. n, the number of inital latents to test\n",
        "2. k, the number of \"top\" latents to pass to hill climbing stage\n",
        "3. t, the number of hill-climbing steps to take\n",
        "\n",
        "Also, if show=True, it will print out the score and display image for\n",
        " each of the top k results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lgfrqtDaN2f"
      },
      "source": [
        "results=searchAndClimb(\"a girl wearing glasses\",1000,10,100,show=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsn_Cf7HbL62"
      },
      "source": [
        "results=searchAndClimb(\"a girl wearing a hat\",100,10,10,show=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkEL1vHShYo1"
      },
      "source": [
        "def displayLatent(latent,phi=1.0):\n",
        "    with torch.no_grad():\n",
        "      img_pt, _ = g(\n",
        "          [torch.from_numpy(latent).to(device)],\n",
        "          truncation=phi,\n",
        "          truncation_latent=latent_avg.to(device),\n",
        "          randomize_noise=False,\n",
        "      )\n",
        "    img=img_pt.cpu().numpy()[0]\n",
        "    return fmtImg(img)\n",
        "\n",
        "def showResults(results):\n",
        "  img = PIL.Image.fromarray(np.hstack([np.array(displayLatent(r)) for (r,s) in results]), 'RGB')\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NTNzlAfbhaXZ"
      },
      "source": [
        "results=searchAndClimb(\"a girl holding an umbrella\",1000,10,100)\n",
        "showResults(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GDRY_Ut8hosR"
      },
      "source": [
        "results=searchAndClimb(\"a girl wearing a white wedding dress\",1000,10,100,show=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUI46zRImhNu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}