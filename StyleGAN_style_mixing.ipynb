{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRxFgHqsBmFqJqN1ecfXzi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/StyleGAN_style_mixing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Style mixing (StyleGAN2)**\n",
        "\n"
      ],
      "metadata": {
        "id": "CQiCEPeYYUfi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2D5qYywYT84",
        "outputId": "84e9b313-9746-4a83-c6a5-9f3e27fd743a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Cloning into 'stylegan2'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Total 328 (delta 0), reused 0 (delta 0), pack-reused 328\u001b[K\n",
            "Receiving objects: 100% (328/328), 10.86 MiB | 7.42 MiB/s, done.\n",
            "Resolving deltas: 100% (172/172), done.\n"
          ]
        }
      ],
      "source": [
        "# --- githubよりコードを取得 ---\n",
        "%tensorflow_version 1.x\n",
        "!git clone https://github.com/cedro3/stylegan2.git\n",
        "#%cd stylegan2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  --- クラスと関数の定義 ---\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import PIL.Image\n",
        "import sys\n",
        "import bz2\n",
        "from keras.utils import get_file\n",
        "import dlib\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "%cd stylegan2\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import projector\n",
        "import pretrained_networks\n",
        "from training import dataset\n",
        "from training import misc\n",
        "\n",
        "# --------- ランドマーク検出 -------------\n",
        "class LandmarksDetector:\n",
        "    def __init__(self, predictor_model_path):\n",
        "        \"\"\"\n",
        "        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file\n",
        "        \"\"\"\n",
        "        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used\n",
        "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
        "\n",
        "    def get_landmarks(self, image):\n",
        "        img = dlib.load_rgb_image(image)\n",
        "        dets = self.detector(img, 1)\n",
        "\n",
        "        for detection in dets:\n",
        "            face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
        "            yield face_landmarks\n",
        "\n",
        "# ---------- 顔画像の切り出し --------------            \n",
        "def image_align(src_file, dst_file, face_landmarks, output_size=1024, transform_size=4096, enable_padding=True):\n",
        "        # Align function from FFHQ dataset pre-processing step\n",
        "        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "        src_file\n",
        "        dst_file \n",
        "        face_landmarks\n",
        "        output_size=1024 \n",
        "        transform_size=4096 \n",
        "        enable_padding=True\n",
        "            \n",
        "        lm = np.array(face_landmarks)\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        y = np.flipud(x) * [-1, 1]\n",
        "        c = eye_avg + eye_to_mouth * 0.1\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "        # Load in-the-wild image.\n",
        "        if not os.path.isfile(src_file):\n",
        "            print('\\nCannot find source image. Please run \"--wilds\" before \"--align\".')\n",
        "            return\n",
        "        img = PIL.Image.open(src_file)\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        img.save(dst_file, 'PNG')\n",
        "\n",
        "\n",
        "# ------------ ファイルの解凍 ---------------        \n",
        "def unpack_bz2(src_path):\n",
        "    data = bz2.BZ2File(src_path).read()\n",
        "    dst_path = src_path[:-4]\n",
        "    with open(dst_path, 'wb') as fp:\n",
        "        fp.write(data)\n",
        "    return dst_path\n",
        "\n",
        "\n",
        "# ------------  潜在変数(ベクトル)の探索　------------\n",
        "def project_real_images(num_images): \n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
        "    dataset_name = 'dataset'  \n",
        "    data_dir = 'my'  \n",
        "    num_snapshots = 5\n",
        "\n",
        "    #**Pretrained_modelをダウンロード**\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    proj = projector.Projector()\n",
        "    proj.set_network(Gs)\n",
        "\n",
        "    #tfrecordからデータをダウンロード\n",
        "    #train/dataset.pyを使用\n",
        "    print('Loading images from \"%s\"...' % dataset_name)\n",
        "    dataset_obj = dataset.load_dataset(data_dir=data_dir, tfrecord_dir=dataset_name, max_label_size=0, repeat=False, shuffle_mb=0)\n",
        "    #print(dataset_obj.shape) #[3, 1024, 1024]\n",
        "    assert dataset_obj.shape == Gs.output_shape[1:]\n",
        "\n",
        "    os.makedirs('my/real_images', exist_ok=True)  \n",
        "    for image_idx in range(num_images):\n",
        "        print('Projecting image %d/%d ...' % (image_idx, num_images))\n",
        "        images, _labels = dataset_obj.get_minibatch_np(1)\n",
        "        images = misc.adjust_dynamic_range(images, [0, 255], [-1, 1]) #train/misc.py\n",
        "        \n",
        "        targets=images\n",
        "        png_prefix='./my/real_images/image'+str(image_idx)\n",
        "        num_snapshots=num_snapshots\n",
        "                \n",
        "        snapshot_steps = set(proj.num_steps - np.linspace(0, proj.num_steps, num_snapshots, endpoint=False, dtype=int))\n",
        "        misc.save_image_grid(targets, png_prefix + 'target.png', drange=[-1,1])\n",
        "        proj.start(targets)\n",
        "        while proj.get_cur_step() < proj.num_steps:\n",
        "            print('\\r%d / %d ... ' % (proj.get_cur_step(), proj.num_steps), end='', flush=True)\n",
        "            proj.step()\n",
        "            if proj.get_cur_step() in snapshot_steps:\n",
        "                misc.save_image_grid(proj.get_images(), png_prefix + 'step%04d.png' % proj.get_cur_step(), drange=[-1,1])\n",
        "            \n",
        "            if proj.get_cur_step() == proj.num_steps:  \n",
        "                vec = proj.get_dlatents() \n",
        "                if image_idx == 0:\n",
        "                   vec_syn = vec\n",
        "                else:\n",
        "                   vec_syn = np.concatenate([vec_syn, vec])  \n",
        "                print(vec_syn.shape)  \n",
        "        print('\\r%-30s\\r' % '', end='', flush=True)\n",
        "\n",
        "    return vec_syn\n",
        "\n",
        "# ------------- 2つのベクトル間を補完するアニメーションの作成 -------\n",
        "def generate_gif(vec_syn, idx):\n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'  \n",
        "    \n",
        "    #pretrained networkをロード\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = True \n",
        "    Gs_syn_kwargs.truncation_psi = 0.5\n",
        "\n",
        "    image_gif = []\n",
        "    image_gif_256 = []\n",
        "    os.makedirs('my/gif', exist_ok=True)\n",
        "    for j in range(len(idx)-1):\n",
        "        for i in range(40):\n",
        "            vec = vec_syn[idx[j]]+(vec_syn[idx[j+1]]-vec_syn[idx[j]])*i/39\n",
        "            vec = vec.reshape(1, 18, 512)\n",
        "            images =  Gs.components.synthesis.run(vec, **Gs_syn_kwargs) \n",
        "            image_one = PIL.Image.fromarray(images[0], 'RGB')\n",
        "            image_gif.append(image_one)\n",
        "            image_gif_256.append(image_one.resize((256,256))) \n",
        "\n",
        "    image_gif[0].save('./my/gif/anime.gif', save_all=True, append_images=image_gif[1:],\n",
        "                      duration=100, loop=0)   \n",
        "    image_gif_256[0].save('./my/gif/anime_256.gif', save_all=True, append_images=image_gif_256[1:],\n",
        "                      duration=100, loop=0) \n",
        "\n",
        "# ------------　 フォルダー内の画像を表示　--------------\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "def display_pic(folder):\n",
        "    fig = plt.figure(figsize=(30, 40))\n",
        "    files = glob.glob(folder)\n",
        "    files.sort()\n",
        "    images=[]\n",
        "    for i in range(len(files)):\n",
        "        img = Image.open(files[i])    \n",
        "        images = np.asarray(img)\n",
        "        ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "        image_plt = np.array(images)\n",
        "        ax.imshow(image_plt)\n",
        "        ax.set_xlabel(str(i), fontsize=20)               \n",
        "    plt.show()\n",
        "    plt.close()  \n",
        "\n",
        "# ------------------ ベクトルの画像化 ---------------\n",
        "def display(vec_syn):\n",
        "  \n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'  \n",
        "    \n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = True \n",
        "    Gs_syn_kwargs.truncation_psi = 0.5\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 40))   \n",
        "    for i in range(len(vec_syn)):\n",
        "        vec = vec_syn[i].reshape(1,18,512)\n",
        "        image =  Gs.components.synthesis.run(vec, **Gs_syn_kwargs)        \n",
        "        PIL.Image.fromarray(image[0], 'RGB')   \n",
        "        ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "        image_plt = np.array(image[0])\n",
        "        ax.imshow(image_plt)\n",
        "        ax.set_xlabel(str(i), fontsize=20)               \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "# make dirctory\n",
        "os.makedirs('my/pic', exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwMFXNNqZSJo",
        "outputId": "427e8818-5520-43d9-f0b4-6b9422c7f352"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 顔画像切り出しモデルの読み込み\n",
        "LANDMARKS_MODEL_URL = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
        "landmarks_model_path = unpack_bz2(get_file('shape_predictor_68_face_landmarks.dat.bz2',\n",
        "                                            LANDMARKS_MODEL_URL, cache_subdir='temp'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnflQAbjZ7X8",
        "outputId": "4a462a8d-a54c-49a1-aced-321d9ff5f785"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "64045056/64040097 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**顔画像の切り出し**\n",
        "\n",
        "sample/picに画像入れる --> my/picに保存される"
      ],
      "metadata": {
        "id": "ePn3D_IaaABy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 顔画像の切り出し\n",
        "RAW_IMAGES_DIR = 'sample/pic'\n",
        "ALIGNED_IMAGES_DIR = 'my/pic'\n",
        "\n",
        "landmarks_detector = LandmarksDetector(landmarks_model_path)\n",
        "for img_name in os.listdir(RAW_IMAGES_DIR):\n",
        "    raw_img_path = os.path.join(RAW_IMAGES_DIR, img_name)\n",
        "    for i, face_landmarks in enumerate(landmarks_detector.get_landmarks(raw_img_path), start=1):\n",
        "        face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], i)\n",
        "        aligned_face_path = os.path.join(ALIGNED_IMAGES_DIR, face_img_name)\n",
        "        image_align(raw_img_path, aligned_face_path, face_landmarks)\n",
        "        \n",
        "display_pic('./my/pic/*.*')"
      ],
      "metadata": {
        "id": "fGyJzFy6aAPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**マルチプル解像度データセットの作成**\n",
        "\n",
        "my/picフォルダーにある顔画像からマルチ解像度のデータセットを作成しmy/datasetフォルダーに保存"
      ],
      "metadata": {
        "id": "C7LMslqma0yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset_tool.py create_from_images ./my/dataset ./my/pic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpsptB6baz39",
        "outputId": "3d3e6336-6e89-4bf2-d097-caf5ca15f42e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from \"./my/pic\"\n",
            "Creating dataset \"./my/dataset\"\n",
            "0 / 5\rdataset_tool.py:86: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 5 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**潜在変数の探索**"
      ],
      "metadata": {
        "id": "TODlzVsdbSEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = len(os.listdir(\"/content/stylegan2/my/pic\"))\n",
        "vec_syn = project_real_images(n_images)  # ()内はマルチ解像度のデータセットを作成した時の画像枚数\n",
        "display_pic('./my/pic/*.*')  # ターゲット画像の表示\n",
        "display(vec_syn)  # 探索した潜在変数によって生成した画像"
      ],
      "metadata": {
        "id": "kuLdq7A3bSOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 探索した潜在変数の保存\n",
        "os.makedirs('my/vector', exist_ok=True)\n",
        "np.save('my/vector/vec_syn', vec_syn)"
      ],
      "metadata": {
        "id": "XmHwAtEYb5Vr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 探索した潜在変数の読み込み\n",
        "vec_syn = np.load('my/vector/vec_syn.npy')"
      ],
      "metadata": {
        "id": "H8WTKFxncP_w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**顔画像のトランジション**\n",
        "\n",
        "変数vec_synに格納されている潜在変数を元に、アニメーションを作成\n",
        "\n",
        "作成した gifアニメーションはmy/real_gifフォルダーに保存"
      ],
      "metadata": {
        "id": "xhpWpa4gcUHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "generate_gif(vec_syn,[1, 0, 3])  # 潜在変数を1番目→0番目→3番目へアニメーション\n",
        "Image('./my/gif/anime_256.gif', format='png')"
      ],
      "metadata": {
        "id": "-xlHyna-cTUx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}