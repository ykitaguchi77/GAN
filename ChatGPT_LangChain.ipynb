{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCKC/7/uJmrkMmqBT6hWXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GAN/blob/master/ChatGPT_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LangChain APP**\n",
        "\n",
        "https://qiita.com/wwwcojp/items/c7f43c5f964b8db8a890\n",
        "\n",
        "https://note.com/npaka/n/n6b7a07e492f1"
      ],
      "metadata": {
        "id": "7sk4E_3TKggH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OfGbm9CHRcG",
        "outputId": "c844888d-c098-49da-a4d1-74c53013c570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# パッケージのインストール\n",
        "!pip install langchain --q\n",
        "!pip install openai --q\n",
        "!pip install google-search-results --q\n",
        "!pip install --upgrade deepl --q\n",
        "import openai\n",
        "import deepl\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "\n",
        "# パッケージのインポート\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# APIキーの準備\n",
        "# #ngrok_aceess_token = key[5]\n",
        "# openai.api_key = key[3]\n",
        "# deepl_auth_key = key[1]\n",
        "# serp_api_key = key[7]\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = key[3]\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key[7]\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = key[9]\n",
        "os.environ[\"GOOGLE_API_KEY\"] = key[11]"
      ],
      "metadata": {
        "id": "Xq40S0FgKt43"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMの準備準備\n",
        "\"\"\"\n",
        "model_name = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
        "\n",
        "\"\"\"\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# ツールの準備\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# エージェントの準備\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "eFZ7FsM5Nb2u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エージェントの実行\n",
        "agent.run(\"おでこにヒアルロン酸を注入する際の合併症は？\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "fO9PAw__Nptt",
        "outputId": "6c873333-89c8-4008-a2aa-451efb1a0461"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI don't understand the question. It appears to be in Japanese. I should use the search tool to translate it and find the answer.\n",
            "Action: Search\n",
            "Action Input: \"Complications of injecting hyaluronic acid into the forehead\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mComplications\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have found information on the complications of injecting hyaluronic acid into the forehead.\n",
            "Final Answer: The complications of injecting hyaluronic acid into the forehead can include bruising, swelling, redness, infection, and allergic reactions.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The complications of injecting hyaluronic acid into the forehead can include bruising, swelling, redness, infection, and allergic reactions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**例を見ながら勉強**"
      ],
      "metadata": {
        "id": "SUlUp0zVVUjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 基本的な入出力"
      ],
      "metadata": {
        "id": "9mNcQ5oCYYuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# モデル名を指定しない場合、「text-davinci-003」が利用されます\n",
        "llm = OpenAI(model_name=\"gpt-4\", temperature=0.9)\n",
        "text = \"国立大学法人東京工業大学と国立大学法人東京医科歯科大学が合併しました。ついクスっと笑ってしまう新名称を考えてください。\"\n",
        "print(llm(prompt=text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z6aNrfhVZfJ",
        "outputId": "44f465a4-ffaa-40f1-9804-9da35a667395"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "国立大学法人東京工医工科大学\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PromptTemplate\n",
        "\n",
        "ユーザー入力や複数出力に対応"
      ],
      "metadata": {
        "id": "Kxp3-xVlXTDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-4\", temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"target\"],\n",
        "    template=\"技術記事のタイトルをよしなに生成してくれるAIの{target}を考えてください。\",\n",
        ")\n",
        "print(llm(prompt=prompt.format(target=\"セールスポイント\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj0WWZaDVm9i",
        "outputId": "514ce0a5-b56f-4c99-f1d3-31dcd566fbe0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 時短効果：記事のタイトルをAIが自動生成することで、著者はより記事の内容に集中することが可能となり、時間の効率化が図れます。\n",
            "\n",
            "2. SEO対策：最新のSEOトレンドに基づいたキーワードやフレーズを組み込んでタイトルを生成することが可能です。\n",
            "\n",
            "3. ユーザーエンゲージメント向上：AIは大量のデータから学習し、読者がクリックしやすいタイトルを生成することで読者のエンゲージメントを高めます。\n",
            "\n",
            "4. 独自の学習アルゴリズム：AIは過去のパフォーマンスに基づいて学習を進め、より効果的なタイトルを生成することができます。\n",
            "\n",
            "5. 一貫性のあるブランドメッセージ：AIは企業やブランドのガイドラインを学習し、一貫性のあるメッセージングを提供します。\n",
            "\n",
            "6. ABテストの容易さ：複数のタイトル案を瞬時に生成し、どのタイトルが最も効果的かをテストすることが容易になります。\n",
            "\n",
            "7. カスタム化能力：使用者の特定のニーズに対応するためのカスタム化が可能で、特定の読者層に対するタイトル生成などができます。 \n",
            "\n",
            "8. 絶え間ない更新：AIは絶えず新しいデータとパターンを学習し続け、時代と共に進化し続ける技術トピックに対するタイトルも最適化されていきます。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        "\n",
        "指定した順番で作業を行う\n",
        "\n",
        "・SimpleSequentialChain : 入出力を1つずつ持つチェーンを繋げるシンプルなチェーン\n",
        "\n",
        "・SequentialChain : 入出力を複数持つチェーンを繋げる一般的なチェーン"
      ],
      "metadata": {
        "id": "aBIHu_dyYTyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "# プロンプトテンプレートの準備\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}を作る新会社名の案を1つ出してください\",\n",
        ")\n",
        "\n",
        "# LLMChainの準備\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# LLMChainの実行\n",
        "print(chain.run(\"家庭用ロボット\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKDNKQZApWFi",
        "outputId": "605a3627-a259-4ecb-8535-ecb03d0d20fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ジョイポット・ファミリー\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SimpleSequentialChainの例2つ"
      ],
      "metadata": {
        "id": "l1t0RAmSrupn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"あなたは新商品の開発を担当しています。新商品のターゲットが与えられたら、ターゲットに売れる新商品のアイデアを出してください。\n",
        "\n",
        "ターゲット: {target}\n",
        "担当者: 以下が新商品のアイデアです。\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"target\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "llm = OpenAI(temperature=.7)\n",
        "template = \"\"\"あなたは経営者です。新商品のアイデアが与えられたら、経営者の観点から批判的にレビューをしてください。\n",
        "\n",
        "アイデア:\n",
        "{idea}\n",
        "経営者：\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"idea\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\n",
        "review = overall_chain.run(\"通勤に1時間以上かかっている20代の男性\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svt7PSlwXHyR",
        "outputId": "1831d528-aba7-4337-8ec1-84fe54a5a1de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "1. 通勤中に使えるワイヤレスノイズキャンセリングイヤホン：1時間以上かかる通勤時間を有効活用できるように、音楽やポッドキャストを聴きながら騒音を効果的にカットしてくれるワイヤレスノイズキャンセリングイヤホンを開発します。\n",
            "\n",
            "2. 通勤時間を有効利用できるスマートフォンアクセサリー：スマートフォンを搭載した小型のアクセサリーを開発します。スマートフォンを装着することで、通勤時間を有効\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "このアイデアをレビューするうえで、まずは市場ニーズや競合状況を十分に把握し、その上で消費者にとって有益であるかを考慮しておく必要があります。ワイヤレスノイズキャンセリングイヤホンに関しては、他社がすでに似た製品を市場に投入している可能性があるため、その点を確認しておく必要があります。また、イヤホンを装着していない場合でも、多くの消費者が騒音を遮断するために特\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://note.com/npaka/n/n61ad59380a43\n",
        "\n",
        "# LLMの定義\n",
        "llm = OpenAI(temperature=.7)\n",
        "\n",
        "\n",
        "# 1つ目のチェーンの準備 : 劇のタイトルからあらすじを生成\n",
        "\n",
        "# テンプレートの準備\n",
        "template = \"\"\"あなたは劇作家です。劇のタイトルが与えられた場合、そのタイトルのあらすじを書くのがあなたの仕事です。\n",
        "\n",
        "タイトル:{title}\n",
        "あらすじ:\"\"\"\n",
        "\n",
        "# プロンプトテンプレートの準備\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"title\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# LLMChainの準備\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "\n",
        "\n",
        "# 2つ目のチェーン : 劇のあらすじからレビューを生成\n",
        "\n",
        "# LLMの準備準備\n",
        "llm = OpenAI(temperature=.7)\n",
        "\n",
        "# テンプレートの準備\n",
        "template = \"\"\"あなたは演劇評論家です。 劇のあらすじが与えられた場合、そのあらすじのレビューを書くのがあなたの仕事です。\n",
        "\n",
        "あらすじ:\n",
        "{synopsis}\n",
        "レビュー:\"\"\"\n",
        "\n",
        "# プロンプトテンプレートの準備\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"synopsis\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# LLMChainの準備\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# SimpleSequentialChainで2つのチェーンを繋ぐ\n",
        "overall_chain = SimpleSequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(overall_chain(\"朝の通勤でほっこりしたこと\"))"
      ],
      "metadata": {
        "id": "Hbfp5KHGqAt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SequentialChainの例（こちらの方が多分使いやすい）"
      ],
      "metadata": {
        "id": "hT8ctD-Ory1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1つ目のチェーン : 劇のタイトルと時代からあらすじを生成\n",
        "\n",
        "# LLMの準備\n",
        "llm = OpenAI(temperature=.7)\n",
        "\n",
        "# テンプレートの準備\n",
        "# テンプレートの準備\n",
        "template = \"\"\"あなたは劇作家です。劇のタイトルと時代が与えられた場合、そのタイトルのあらすじを書くのがあなたの仕事です。\n",
        "\n",
        "タイトル:{title}\n",
        "時代:{era}\n",
        "あらすじ:\"\"\"\n",
        "\n",
        "# プロンプトテンプレートの生成\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"title\", 'era'],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# LLMChainの準備\n",
        "synopsis_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    output_key=\"synopsis\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2つ目のチェーン : 劇のあらすじからレビューを生成\n",
        "\n",
        "# LLMの準備準備\n",
        "llm = OpenAI(temperature=.7)\n",
        "\n",
        "# テンプレートの準備\n",
        "template = \"\"\"あなたは演劇評論家です。 劇のあらすじが与えられた場合、そのあらすじのレビューを書くのがあなたの仕事です。\n",
        "\n",
        "あらすじ:\n",
        "{synopsis}\n",
        "レビュー:\"\"\"\n",
        "\n",
        "# プロンプトテンプレートの準備\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"synopsis\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# LLMChainの準備\n",
        "review_chain = LLMChain(\n",
        "    llm=llm, prompt=prompt_template,\n",
        "    output_key=\"review\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# SequentialChainで2つのチェーンを繋ぐ\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        "    input_variables=[\"era\", \"title\"],\n",
        "    output_variables=[\"synopsis\", \"review\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "DfLDVLEwsBQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SequentialChainの実行\n",
        "review = overall_chain({\"title\":\"通勤時にほっこりしたこと\", \"era\": \"戦国時代\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6wciuJ_sXb-",
        "outputId": "4520de34-728a-43b9-84d0-e8a9264f9c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"あらすじ：{review['synopsis']}\")\n",
        "print(\"\")\n",
        "print(f\"評論：{review['review']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rolKqaPZtTF2",
        "outputId": "14094781-7739-44f4-a313-d86a943c0c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "あらすじ：\n",
            "\n",
            "戦国時代の朝、ある農家の娘と彼女の叔父が、彼女を大都市に連れて行く旅をしていました。彼女は驚いて、そこで何が起こるのか想像できませんでした。着いた先は、大きな城が立ち並ぶ大都市でした。そして娘は彼女の叔父と同じように、農場から大都市への旅を毎朝、通勤する農民の姿を見ていました。農民たちは行動することで、自分の夢を叶えようとしていました。そして、\n",
            "\n",
            "評論：\n",
            "\n",
            "この作品は、戦国時代の農家の娘が、叔父と一緒に大都市への旅をし、そこで何が起きるのかを想像できない状況にあるという物語です。この作品の主人公は、農民たちが毎朝行動を起こして、自分の夢を叶えようとしている姿を見て、勇気を得ることができます。作品全体は、戦国時代の大都市の生活を熱望する農民たちの姿を描写しています。この作品は、勇気を持って夢を\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "Chainをどのような順番で動かすかを決定する\n",
        "\n",
        "・zero-shot-react-description：  Toolのdescription（説明文）をもとに、どのToolを選択するか決定する\n",
        "\n",
        "・react-docstore:  ドキュメントを検索するSearchと、検索結果から用語を探すLookupという2つのToolを用意\n",
        "\n",
        "・self-ask-with-search:  Google検索等を用いて、自問自答・検索を繰り返しながら答えを導く\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5wOEG9hraXZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import LLMMathChain, SerpAPIWrapper\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "search = SerpAPIWrapper()\n",
        "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=llm_math_chain.run,\n",
        "        description=\"useful for when you need to answer questions about math\"\n",
        "    )\n",
        "]\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "agent.run(\"日本で一番高い山の高さは何メートルですか?その高さの5乗根を求めてください。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "0xn9d4mCbLTJ",
        "outputId": "63a855b8-8730-430a-849e-e10b31712e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find the height of the highest mountain in Japan and then calculate its fifth root.\n",
            "Action: Search\n",
            "Action Input: \"highest mountain in Japan\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mMount Fuji\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find the height of Mount Fuji\n",
            "Action: Search\n",
            "Action Input: \"height of Mount Fuji\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m12,388′\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to convert this to meters\n",
            "Action: Calculator\n",
            "Action Input: 12,388 * 0.3048\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "12,388 * 0.3048\u001b[32;1m\u001b[1;3m\n",
            "Answer: 3760.544\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3760.544\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the fifth root of 3760.544\n",
            "Action: Calculator\n",
            "Action Input: 3760.544^(1/5)\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "3760.544^(1/5)\u001b[32;1m\u001b[1;3m\n",
            "```python\n",
            "print(3760.544**(1/5))\n",
            "```\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m5.188599325914775\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 5.188599325914775\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The height of the highest mountain in Japan is 3760.544 meters and its fifth root is 5.188599325914775.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The height of the highest mountain in Japan is 3760.544 meters and its fifth root is 5.188599325914775.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory\n",
        "\n",
        "前までの会話を覚えておく\n",
        "\n",
        "・  ConversationBufferMemory  前段までの入力・出力をそのまま記憶する\n",
        "\n",
        "・  ConversationalBufferWindowMemory  k段前までの入力・出力をそのまま記憶する\n",
        "\n",
        "・  ConversationSummaryMemory  前段までの入力・出力をLLMでサマライズして記憶する"
      ],
      "metadata": {
        "id": "yDlM89GFcodW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatOpenAI GPT 3.5\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "# Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "# env に読み込ませるAPIキーの類\n",
        "# 環境変数にAPIキーを設定\n",
        "import os\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "# Memory の作成と参照の獲得\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "uVv_iTVqxcZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1dOTDJGxcis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTGDrKwsxckw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Google custom searchを併用する**\n",
        "\n",
        "https://note.com/npaka/n/nd9a4a26a8932"
      ],
      "metadata": {
        "id": "KvvPSSpliGGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor, load_tools\n",
        "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
        "\n",
        "# ツールの準備\n",
        "tools = load_tools([\"google-search\"], llm=OpenAI())"
      ],
      "metadata": {
        "id": "IrZazoRqiKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#テンプレートの準備\n",
        "prefix = \"\"\"次の質問にできる限り答えてください。次のツールにアクセスできます:\"\"\"\n",
        "suffix = \"\"\"始めましょう! 最終的な答えを出すときは、一人称は\"ぼく\"、語尾には\"なのだ\"を使用してください\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\", \"agent_scratchpad\"]\n",
        ")\n",
        "\n",
        "# エージェントの準備\n",
        "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "0iugockNiYZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"天才バカボンのパパが卒業した大学は？\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "AYl3Hb4rick7",
        "outputId": "54c218ab-6477-4aa5-a9fa-315889a53ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: パパの卒業大学を探す\n",
            "Action: Google Search\n",
            "Action Input: 天才バカボン パパ 卒業大学\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJul 31, 2018 ... バカボンのパパが「バカ田大学」を卒業（哲学科の首席！）したことは有名な話ですが、その在学中には「黒百合女学院」の“マドンナ”（のちのママ）に恋 ... バカボンのパパ（バカボンパパ）は、赤塚不二夫のギャグ漫画・『天才バカボン』、ならびにそれを ... 最終学歴, バカ田大学 ... 天才バカボン』を第5作と記す。 天才バカボン キャラ紹介. パパバカ田大学を主席で卒業するも、定職に就かず「これでいいのだ！ ... 時に、パパを懸命に怒っている姿さえも愛らしい。 バカ田大学とは赤塚不二夫のギャグ漫画「天才バカボン」に登場する大学。バカボンのパパを輩出したことで有名。 概要 バカ田大学とは赤塚不二夫のギャグ漫画「天才 ... 天才バカボンのパパは、熊本県に実在する菊地市立七城中学校から東京都のバカ田高校を経て、バカ田大学を主席で卒業。かなり多くの職を転々としていますが、店舗を全焼 ... バカ田大学社会学部哲学科出身です。東京の西北早稲田の隣です。生徒の中にはアホウドリもいます。 姉妹校には「アッホー大学」、「バーカード大学」があります。 Sep 11, 2008 ... 天才バカボンの大神秘―バカボンのパパの知能指数は12500なのだ!?作者: バカ田大学後援会出版社/メーカー: ベストセラーズ発売日: 1993/06メディア: ... 名称は早稲田大学のパロディであり、校歌の歌詞もモデルの「都の西北早稲田の森に～」をもじり、「都の東北早稲田の隣～」となっている。バカボンのパパが首席で卒業した ... バカボンのパパは、熊本県金地郡七城町立七城中学校、東京都のバカ田高校を経てバカ田大学を首席で卒業。学級委員も務めています。/ バカボンのパパ（バカボンパパ）は、赤塚不二夫のギャグ漫画・『天才 ... バカ田大学の卒業式の日、「東洋工業（現・マツダ）に入社してマツダ・キャロルを作るのだ」 ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m パパが卒業した大学はバカ田大学なのだ\n",
            "Final Answer: パパはバカ田大学を首席で卒業したなのだ。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'パパはバカ田大学を首席で卒業したなのだ。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 危ないアドレスかどうかを判断する\n",
        "\n",
        "※これはColabでは動きません...\n",
        "\n",
        "https://zenn.dev/tatsui/articles/c4b4f796a85395"
      ],
      "metadata": {
        "id": "rDAqFsqhjFec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import click\n",
        "\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI, SerpAPIWrapper, LLMChain, LLMMathChain, LLMBashChain\n",
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor, load_tools, initialize_agent\n",
        "\n",
        "def generate_prompt(identifier:str,input_value:str):\n",
        "    if identifier in [\"url\", \"domain\"]:\n",
        "        prompt = f\"\"\"\n",
        "以下の手順を使ってURL({input_value})の調査を行い、悪意の有無について「日本語」で出力してください。\n",
        "* digコマンドを使って入力されたドメイン名からIPアドレスを取得します。\n",
        "* whoisコマンドを使ってWhois情報を取得します。\n",
        "* IPアドレスをgoogleで検索する\n",
        "\n",
        "判定基準\n",
        "* IP Abuse Reports\n",
        "\"\"\"\n",
        "\n",
        "    if identifier == \"ip\":\n",
        "        prompt = f\"\"\"\n",
        "以下の手順を使ってIPアドレス({input_value})の調査を行い、悪意の有無について「日本語」で出力してください。\n",
        "* whoisコマンドを使ってWhois情報を取得します。\n",
        "* IPアドレスをgoogleで検索する\n",
        "\n",
        "判定基準\n",
        "* 悪意のあるIPアドレスとして報告されている\n",
        "* IP Abuse Reports\n",
        "\"\"\"\n",
        "\n",
        "    if identifier == \"hash\":\n",
        "       prompt = f\"\"\"\n",
        "以下の手順を使ってHash({input_value})の調査を行い、悪意の有無について「日本語」で出力してください。\n",
        "* ハッシュ値をGoogle検索する\n",
        "\n",
        "判定基準\n",
        "* 悪意のあるファイルとして報告されている\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "@click.command()\n",
        "@click.option(\"--url\", \"identifier\", flag_value=\"url\", help=\"Investigate based on URL.\")\n",
        "@click.option(\"--hash\", \"identifier\", flag_value=\"hash\", help=\"Investigate based on file hash.\")\n",
        "@click.option(\"--domain\", \"identifier\", flag_value=\"domain\", help=\"Investigate based on domain name.\")\n",
        "@click.option(\"--ip\", \"identifier\", flag_value=\"ip\", help=\"Investigate based on IP address.\")\n",
        "@click.argument(\"input_value\")\n",
        "def main(identifier: str, input_value: str):\n",
        "    # ツールの準備\n",
        "    llm = OpenAI(temperature=0)\n",
        "    tools = load_tools([\"terminal\", \"google-search\"], llm=llm)\n",
        "    llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
        "    llm_bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "    tools.append(\n",
        "        Tool(\n",
        "            name=\"Calculator\",\n",
        "            func=llm_math_chain.run,\n",
        "            description=\"useful for when you need to answer questions about math\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    prompt = generate_prompt(identifier, input_value)\n",
        "    agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "    agent.run(prompt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "854EqcbejH3J",
        "outputId": "ba414788-4c08-40d9-aee6-c4f32ec00122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Usage: ipykernel_launcher.py [OPTIONS] INPUT_VALUE\n",
            "Try 'ipykernel_launcher.py --help' for help.\n",
            "\n",
            "Error: no such option: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**AI chatbot**\n",
        "\n",
        "https://qiita.com/hideki/items/b26154ab503fd3394a0b"
      ],
      "metadata": {
        "id": "MzryN14JktCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --q\n",
        "!pip install openai --q\n",
        "!pip install gradio --q"
      ],
      "metadata": {
        "id": "kFSaw2kNkvUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = key[3]\n",
        "\n",
        "llm = OpenAI(temperature=0.95)\n",
        "conversation = ConversationChain(llm=llm, verbose=False)\n",
        "\n",
        "def chat(message, history):\n",
        "    history = history or []\n",
        "    response = conversation.predict(input=message)\n",
        "    history.append((message, response))\n",
        "\n",
        "    return history, history\n",
        "\n",
        "\n",
        "chatbot = gr.Chatbot().style(color_map=('green', 'pink'))\n",
        "demo = gr.Interface(\n",
        "    chat,\n",
        "    ['text', 'state'],\n",
        "    [chatbot, 'state'],\n",
        "    allow_flagging='never',\n",
        ")\n",
        "if __name__ == '__main__':\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "wcv2bqzykzPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### コードエンジニアリングをしてみる"
      ],
      "metadata": {
        "id": "BOCT2Vj1mCWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from langchain import OpenAI, ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = key[3]\n",
        "\n",
        "llm = OpenAI(temperature=1)\n",
        "conversation = ConversationChain(llm=llm, verbose=False)\n",
        "\n",
        "template = '''I need you to act as a research assistant for a scientific article I am writing. I will provide you with specific questions and information, and I expect you to find relevant and credible sources to support my research. Your responses should include a summary of the main points from the sources, along with the full citation in the desired format (APA, MLA, etc.). Additionally, you should flag any potential issues or limitations in the sources you provide. The title of the article is: Real-time smartphone identification of corneal diseases displayed on computer screen  using embedded deep-learning model: a validation study\n",
        "\n",
        "{history}\n",
        "{input}\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['history', 'input'],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True)\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    history = history or []\n",
        "    response = conversation.predict(input=message)\n",
        "    history.append((message, response))\n",
        "\n",
        "    return history, history\n",
        "\n",
        "\n",
        "chatbot = gr.Chatbot().style(color_map=('green', 'pink'))\n",
        "demo = gr.Interface(\n",
        "    chat,\n",
        "    ['text', 'state'],\n",
        "    [chatbot, 'state'],\n",
        "    allow_flagging='never',\n",
        ")\n",
        "if __name__ == '__main__':\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "sRGf-3m-luV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://qiita.com/KENTAROSZK/items/33df89174928f54ae44c"
      ],
      "metadata": {
        "id": "uYFbfXG5z3YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "NZGgBaV2z3oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要ライブラリのimport\n",
        "import os\n",
        "# gradioのインポート\n",
        "import gradio as gr\n",
        "\n",
        "# langchain関連のパッケージインポート\n",
        "\n",
        "# エージェント系のライブラリインポート\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# OpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 会話用のメモリ\n",
        "from langchain import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory # 要約しながら会話をするとき\n",
        "from langchain.chains.conversation.memory import ConversationalBufferWindowMemory # ある時点までの過去の会話を記憶しながら会話をするとき\n",
        "\n",
        "\n",
        "# ChatGPTっぽく使うためにはプロンプトが必要なので\n",
        "from langchain.agents  import ZeroShotAgent\n",
        "\n",
        "\n",
        "from langchain.agents  import AgentExecutor\n",
        "\n",
        "\n",
        "from langchain.chains  import LLMChain"
      ],
      "metadata": {
        "id": "P6JYKCooz8gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = key[3]\n",
        "os.environ[\"SERPAPI_API_KEY\"] = key[7]\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = key[9]\n",
        "os.environ[\"GOOGLE_API_KEY\"] = key[11]"
      ],
      "metadata": {
        "id": "70Gwm3pfz-gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMの定義\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# 利用するツールの定義\n",
        "# ※llm-mathは必須だから加えておく\n",
        "tools = load_tools([\"google-search\", \"llm-math\"],\n",
        "                   llm = llm)"
      ],
      "metadata": {
        "id": "NS7CCNSO0HRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# プロンプトの生成&定義\n",
        "prefix = \"\"\" Have a conversation with a human, answering the following questions as best you can. You have access to the following tools: \"\"\"\n",
        "\n",
        "suffix = \"\"\"Begin!\"\n",
        "            {chat_history}\n",
        "            Question: {input}\n",
        "            {agent_scratchpad}\n",
        "         \"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(tools,\n",
        "                                     prefix = prefix,\n",
        "                                     suffix = suffix,\n",
        "                                     input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"])"
      ],
      "metadata": {
        "id": "Ws-IT72p0RST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Chainの定義\n",
        "llm_chain = LLMChain(llm    = llm,\n",
        "                     prompt = prompt)"
      ],
      "metadata": {
        "id": "Qzhjmbtj0XMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# エージェントのインスタンス化\n",
        "\n",
        "agent = ZeroShotAgent(llm_chain = llm_chain,\n",
        "                      tools     = tools,\n",
        "                      verbose   = True)"
      ],
      "metadata": {
        "id": "dt7ktpSk0Ze6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# メモリの定義\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "7n4k2Hf40br7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定義したエージェントやツール、メモリを使って、エージェントのチェーンを生成\n",
        "\n",
        "agent_chain = AgentExecutor.from_agent_and_tools(agent   = agent,\n",
        "                                                 tools   = tools,\n",
        "                                                 verbose = True,\n",
        "                                                 memory  = memory\n",
        "                                                 )"
      ],
      "metadata": {
        "id": "441PByf30eGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradioの設定\n",
        "\n",
        "def chat(message, history):\n",
        "  history  = history or []\n",
        "  response = agent_chain.run(input = message) # 人間が入力したテキストをmessageとして受け取って、responseを返す\n",
        "\n",
        "  history.append((message, response))\n",
        "\n",
        "  return history, history\n",
        "\n",
        "# 見た目の設定\n",
        "chatbot = gr.Chatbot().style(color_map=('green', 'pink'))\n",
        "\n",
        "demo = gr.Interface(\n",
        "    chat,\n",
        "    ['text',  'state'],\n",
        "    [chatbot, 'state'],\n",
        "    allow_flagging = 'never',\n",
        ")"
      ],
      "metadata": {
        "id": "nJYQyfvM0hPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "2dxUCASW0mWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}